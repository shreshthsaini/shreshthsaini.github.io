<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Shreshth Saini Â· Generative AI &amp; Perceptual Quality Research</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Shreshth Saini is a PhD candidate at UT Austin advancing the theory and applications of generative models, HDR video quality, and perceptual AI.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/UT_profile.png">
</head>
<body>
  <div class="page-shell">
    <header class="site-header">
      <a href="index.html" class="brand">
        <span class="brand-mark">SS</span>
        <span class="brand-text">
          <span class="brand-title">Shreshth Saini</span>
          <span class="brand-subtitle">Generative AI â€¢ Perceptual Quality</span>
        </span>
      </a>
      <nav class="site-nav">
        <a href="#about">About</a>
        <a href="#updates">Updates</a>
        <a href="#experience">Experience</a>
        <a href="#services">Service</a>
        <a href="#research">Research</a>
        <a href="#projects">Projects</a>
        <a href="#contact">Contact</a>
        <a href="blogs.html" class="nav-button">Blog</a>
      </nav>
    </header>

    <main>
      <section id="about" class="panel panel-hero">
        <div class="hero-grid">
          <div class="hero-body">
            <p class="eyebrow">PhD candidate Â· Laboratory for Image &amp; Video Engineering</p>
            <h1 class="hero-title">Designing trustworthy generative systems for imagery &amp; video.</h1>
            <p class="hero-lede">I am Shreshth Saini, a third-year PhD student at the <a href="https://www.utexas.edu/">University of Texas at Austin</a> working within the <a href="https://live.ece.utexas.edu/">Laboratory for Image and Video Engineering</a>, where I am advised by <a href="https://www.ece.utexas.edu/people/faculty/alan-bovik">Prof. Alan C. Bovik</a>.</p>
            <p>My research focuses on the theoretical foundations of generative modelsâ€”flows, diffusion models, and multimodal large language modelsâ€”and how these systems can be harnessed for efficient sampling, perceptual image/video quality assessment, editing, and inverse problems such as inverse tone mapping.</p>
            <p>I collaborate with the YouTube/Google Media Algorithms team during my PhD, and I will join <span class="tag tag-highlight">Google Research</span> as a student researcher on the LUMA team starting June 2025.</p>
            <p>Before UT Austin, I built medical imaging products as a Research Engineerâ€“AI at Arkray, Inc. and as a Machine Learning Engineer at BioMind AI. I received my Bachelor's degree in Electrical Engineering from <a href="https://www.iitj.ac.in/">IIT Jodhpur</a>, where I was advised by <a href="https://www.mornin-feng.com/">Prof. Mengling Feng</a>, <a href="https://faculty.iitmandi.ac.in/~aditya/">Prof. Aditya Nigam</a>, and <a href="https://research.iitj.ac.in/researcher/anil-kumar-tiwari">Prof. Anil K. Tiwari</a>.</p>
            <div class="hero-tags">
              <span class="tag">Diffusion &amp; Flow Models</span>
              <span class="tag">Perceptual IQA &amp; VQA</span>
              <span class="tag">HDR Video</span>
              <span class="tag">Generative Editing</span>
              <span class="tag">Inverse Problems</span>
              <span class="tag">Medical Imaging</span>
            </div>
            <ul class="hero-links">
              <li><a href="mailto:saini.2@utexas.edu">Contact</a></li>
              <li><a href="https://scholar.google.co.in/citations?user=OpZ-5K4AAAAJ&hl=en">Google Scholar</a></li>
              <li><a href="https://www.linkedin.com/in/shreshthfeel/">LinkedIn</a></li>
              <li><a href="https://x.com/shreshthsaini">X</a></li>
              <li><a href="https://github.com/shreshthsaini">GitHub</a></li>
              <li><a href="data/cv.pdf">CV (Dated ðŸ˜ž)</a></li>
            </ul>
          </div>
          <div class="hero-image">
            <div class="portrait-frame">
              <img src="images/shreshth-portrait.jpeg" alt="Portrait of Shreshth Saini">
            </div>
          </div>
        </div>
      </section>

      <section id="updates" class="panel">
        <div class="section-heading">
          <h2>Signals &amp; updates</h2>
          <p class="section-subtitle">Recent milestones from the lab, collaborations, and community.</p>
        </div>
        <ul class="timeline">
          <li class="timeline-item">
            <span class="timeline-date">Sept 2025</span>
            <div class="timeline-body"><strong>Rectified CFG++</strong> accepted to <strong>NeurIPS 2025</strong>!</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">Jun 2025</span>
            <div class="timeline-body">Excited to join <strong>Google Research</strong> as a student researcher with the LUMA team.</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">May 2025</span>
            <div class="timeline-body">Paper accepted at <strong>IEEE ICIP 2025</strong>.</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">May 2025</span>
            <div class="timeline-body">Paper accepted at the <strong>42nd International Conference on Machine Learning (ICML)</strong> 2025.</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">May 2025</span>
            <div class="timeline-body">Paper accepted at the <strong>CVPR 2025</strong> Workshop on Generative Models for Computer Vision.</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">Mar 2025</span>
            <div class="timeline-body">Honored to be appointed <strong>Assistant Director</strong> of <a href="https://live.ece.utexas.edu/">LIVE at UT Austin</a>.</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">Jun 2024</span>
            <div class="timeline-body">Joined <strong>Amazon</strong> as an Applied Scientist-II Intern in the Perception team.</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">Jan 2024</span>
            <div class="timeline-body">Joined <strong>Alibaba US</strong> as a Research Intern to work on diffusion models.</div>
          </li>
          <li class="timeline-item">
            <span class="timeline-date">Nov 2023</span>
            <div class="timeline-body">Paper accepted to the <strong>WACV 2024</strong> Workshop on Image/Video/Audio Quality in CV and Gen AI.</div>
          </li>
        </ul>
        <details class="timeline-archive">
          <summary>Earlier milestones</summary>
          <ul>
            <li class="timeline-item">
              <span class="timeline-date">Jun 2023</span>
              <div class="timeline-body">Completed the first-ever large-scale HDR subjective perceptual quality study on Amazon Mechanical Turk.</div>
            </li>
            <li class="timeline-item">
              <span class="timeline-date">Aug 2022</span>
              <div class="timeline-body">Joined <strong>LIVE at UT Austin</strong> as a PhD student under Prof. Alan C. Bovik.</div>
            </li>
            <li class="timeline-item">
              <span class="timeline-date">Aug 2022</span>
              <div class="timeline-body">Awarded the Engineering Graduate Fellowship through 2027.</div>
            </li>
            <li class="timeline-item">
              <span class="timeline-date">Feb 2022</span>
              <div class="timeline-body">Joined <strong>BioMind</strong>, Singapore as a Research Engineer / Machine Learning Engineer.</div>
            </li>
            <li class="timeline-item">
              <span class="timeline-date">Aug 2020</span>
              <div class="timeline-body">Started as a Research Engineer (AI) at <strong>Arkray, Inc.</strong></div>
            </li>
            <li class="timeline-item">
              <span class="timeline-date">May 2019</span>
              <div class="timeline-body">Research Assistant at the <strong>National University of Singapore</strong> under Dr. Mengling "Mornin" Feng.</div>
            </li>
            <li class="timeline-item">
              <span class="timeline-date">Aug 2018</span>
              <div class="timeline-body">Undergraduate Researcher at the Image Processing and Computer Vision Lab, IIT Jodhpur.</div>
            </li>
            <li class="timeline-item">
              <span class="timeline-date">May 2018</span>
              <div class="timeline-body">Research Intern at The Multimedia Analytics, Networks and Systems Lab, IIT Mandi.</div>
            </li>
          </ul>
        </details>
      </section>

      <section id="experience" class="panel">
        <div class="section-heading">
          <h2>Research &amp; development experience</h2>
          <p class="section-subtitle">Roles spanning academia and industry where I build perceptual quality systems, generative models, and intelligent tooling.</p>
        </div>
        <div class="experience-grid">
          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Student Researcher Â· Google Research â€” LUMA Team</h3>
                <p class="experience-meta">Mountain View, California Â· June 2025 â€“ Present</p>
              </div>
              <span class="experience-tag">Generative AI</span>
            </header>
            <p>Exploring generative editing and high-fidelity media creation with the LUMA team.</p>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Graduate Research Assistant Â· Laboratory for Image &amp; Video Engineering</h3>
                <p class="experience-meta">Austin, Texas Â· August 2022 â€“ Present</p>
              </div>
              <span class="experience-tag">Perceptual QA</span>
            </header>
            <p>Developing the next generation of HDR-native generative and quality assessment systems at UT Austin.</p>
            <details>
              <summary>Key workstreams</summary>
              <ul>
                <li>Developing scalable vision models for HDR videos spanning inverse tone mapping, gamut expansion, and quality assessment.</li>
                <li>Created the largest HDR-SDR dataset for short-form videos (publicly available).</li>
                <li>Designing video quality assessment methods for HDR videos using non-linear expansion of extremes of sub-level luminance.</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Applied Scientist Intern Â· Amazon â€” Perception Team</h3>
                <p class="experience-meta">Seattle, Washington Â· June 2024 â€“ August 2024</p>
              </div>
              <span class="experience-tag">Synthetic Data</span>
            </header>
            <p>Advanced perception research with large-scale synthetic data generation at Amazon.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Built large-scale synthetic data generation pipelines with the Perception team.</li>
                <li>Developed a novel edit-bench and text-to-image diffusion model for consistent image/video editing and generation.</li>
                <li>Initiated efforts toward an image + video editing challenge and workshop.</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Research Intern Â· Alibaba Group</h3>
                <p class="experience-meta">Sunnyvale, California Â· January 2024 â€“ May 2024</p>
              </div>
              <span class="experience-tag">Diffusion Models</span>
            </header>
            <p>Researched robust vision model-based video quality assessment at Alibaba.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Developed generalizable and robust video quality assessment methods driven by vision models.</li>
                <li>Leveraged diffusion model priors as perceptual consistency signals for image quality assessment.</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Co-Founder Â· Short-X</h3>
                <p class="experience-meta">Austin, Texas Â· January 2023 â€“ January 2024</p>
              </div>
              <span class="experience-tag">Startup</span>
            </header>
            <p>Co-founded Short-X to automate the creation of engaging short-form media from long-form content.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Designed AI pipelines for transcription, highlight extraction, and smart vertical cropping.</li>
                <li>Built models for pause removal, speaker identification, and unique moment detection.</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Machine Learning Engineer Â· BioMind</h3>
                <p class="experience-meta">Singapore Â· February 2022 â€“ June 2022</p>
              </div>
              <span class="experience-tag">Medical AI</span>
            </header>
            <p>Advanced multi-modal medical imaging models for BioMind products.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Developed state-of-the-art multimodal deep learning models for 25+ tumor and non-tumor classes.</li>
                <li>Employed TFRecords for memory-intensive 4D datasets and proposed multi-task models for tumor predictions.</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Research Engineer â€“ AI Â· Arkray, Inc.</h3>
                <p class="experience-meta">Kyoto, Japan (Remote) Â· August 2020 â€“ December 2021</p>
              </div>
              <span class="experience-tag">Medical Imaging</span>
            </header>
            <p>Delivered AI solutions for diagnostic devices across Arkrayâ€™s portfolio.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Proposed semi-supervised models to learn from large, noisy, and partially labeled 2D datasets.</li>
                <li>Deployed models for the UrineSediment Analyzer and automated BodyFluid Analyzer (Aution EYE).</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Research Assistant Â· National University of Singapore</h3>
                <p class="experience-meta">Singapore Â· May 2019 â€“ July 2019</p>
              </div>
              <span class="experience-tag">Healthcare AI</span>
            </header>
            <p>Developed deep learning architectures for large-scale public health datasets.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Designed novel deep learning models tailored for large-scale public health data.</li>
                <li>Published state-of-the-art results for skin lesion analysis with low computational cost.</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Undergraduate Researcher Â· Image Processing &amp; Computer Vision Lab, IIT Jodhpur</h3>
                <p class="experience-meta">Jodhpur, India Â· August 2018 â€“ August 2020</p>
              </div>
              <span class="experience-tag">Research</span>
            </header>
            <p>Prototyped machine learning methods for diagnosis and treatment support.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Developed deep learning models for retinal vessel &amp; skin lesion segmentation, and left-atrium diagnosis in 3D GE-MRIs.</li>
                <li>Explored AI-based diagnosis and treatment planning workflows.</li>
              </ul>
            </details>
          </article>

          <article class="experience-card">
            <header>
              <div>
                <h3 class="experience-title">Research Intern Â· The Multimedia Analytics, Networks and Systems Lab, IIT Mandi</h3>
                <p class="experience-meta">Mandi, India Â· May 2018 â€“ July 2018</p>
              </div>
              <span class="experience-tag">Computer Vision</span>
            </header>
            <p>Investigated cascaded hourglass architectures for iris segmentation.</p>
            <details>
              <summary>Highlights</summary>
              <ul>
                <li>Developed a cascaded hourglass CNN for iris segmentation within an encoderâ€“decoder design.</li>
              </ul>
            </details>
          </article>
        </div>
      </section>

      <section id="services" class="panel">
        <div class="section-heading">
          <h2>Service &amp; leadership</h2>
          <p class="section-subtitle">Giving back to the research community and mentoring next-generation builders.</p>
        </div>
        <p class="service-list">
          <strong>Reviewer:</strong> ICLR (2026, 2025), IEEE Transactions on Multimedia (2024, 2025), ICML (2025), CVPR (2026, 2025), WACV (2026, 2025), IEEE Transactions on Image Processing (2025).<br>
          <strong>Assistant Director:</strong> <a href="https://live.ece.utexas.edu/">LIVE at UT Austin</a> (2025 â€“ Present).
        </p>
        <details class="details-block">
          <summary>Community roles prior to UT Austin</summary>
          <ul>
            <li>Volunteer at the Internal Workshop on Deep Learning (IWDL), India (2018).</li>
            <li>Established and ran the LAMBDA Lab at IIT Jodhpur (2018 â€“ 2020).</li>
            <li>Overall Head of Entrepreneurship and Innovation Cell at IIT Jodhpur (2018 â€“ 2019).</li>
            <li>Assistant Head of Counselling Services at IIT Jodhpur (2018 â€“ 2019).</li>
          </ul>
        </details>
      </section>

      <section id="research" class="panel">
        <div class="section-heading">
          <h2>Research highlights</h2>
          <p class="section-subtitle">Recent work at the intersection of generative modeling, perceptual quality, and HDR video understanding.</p>
        </div>
        <div class="publication-grid">
          <article class="publication-card">
            <figure>
              <img src="images/rect-cfg.png" alt="Rectified CFG++ illustration">
            </figure>
            <div>
              <h3><a href="https://shreshthsaini.github.io/Rectified-CFGpp/">Rectified-CFG++ for Flow-based Models</a></h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, S. Gupta, A. C. Bovik Â· NeurIPS 2025</p>
              <p>Rectified CFG++ enhances conditional image generation with Rectified Flow models by adaptively correcting the latent trajectory, improving visual coherence and prompt alignment.</p>
              <div class="publication-links">
                <a href="https://shreshthsaini.github.io/Rectified-CFGpp/data/pdfs/Rect_CFGpp_Neurips-compressed-names.pdf">Paper</a>
                <a href="https://neurips.cc/virtual/2025/poster/118333">NeurIPS</a>
                <a href="https://shreshthsaini.github.io/Rectified-CFGpp/">Project</a>
                <a href="https://github.com/shreshthsaini/Rectified-CFGpp">Code</a>
              </div>
            </div>
          </article>

          <article class="publication-card">
            <figure>
              <img src="images/pcdm.png" alt="LGDM diffusion guidance visualization">
            </figure>
            <div>
              <h3><a href="data/camera_ready_LGDM.pdf">LGDM: Latent Guidance in Diffusion Models for Perceptual Evaluations</a></h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, R. Liao, Y. Ye, A. C. Bovik Â· ICML 2025</p>
              <p>Investigates how diffusion model priors can serve as perceptual consistency signals, aligning objective image quality assessment with human perception.</p>
              <div class="publication-links">
                <a href="data/camera_ready_LGDM.pdf">Camera Ready</a>
                <a href="https://openreview.net/pdf?id=lIdcR7vU76">OpenReview</a>
                <a href="https://arxiv.org/abs/2506.00327">arXiv</a>
              </div>
            </div>
          </article>

          <article class="publication-card">
            <figure>
              <img src="images/ugc-hdr-mllm.png" alt="Reasoning through perceptual quality for HDR videos graphic">
            </figure>
            <div>
              <h3>Reasoning Through Perceptual Quality for UGC-HDR Videos</h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, N. Birkbeck, Y. Wang, B. Adsumilli, A. C. Bovik Â· Under review (ICLR 2026)</p>
              <p>Introduces a 40K UGC-HDR subjective video quality database and an HDR-native reasoning MLLM for perceptual video quality assessment.</p>
              <div class="publication-links">
                <span class="link-disabled">Preprint coming soon</span>
              </div>
            </div>
          </article>

          <article class="publication-card">
            <figure>
              <img src="images/brightrate.png" alt="BrightRate HDR quality assessment visualization">
            </figure>
            <div>
              <h3><a href="https://brightvqa.github.io/BrightVQ/">BrightRate: Quality Assessment for User-Generated HDR Videos</a></h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, B. Chen, N. Birkbeck, Y. Wang, B. Adsumilli, A. C. Bovik Â· Under review (WACV 2025)</p>
              <p>Designs a reliable HDR video quality metric tailored to the variability of user-generated content, bolstered by a new evaluation benchmark.</p>
              <div class="publication-links">
                <a href="https://brightvqa.github.io/BrightVQ/static/pdfs/BrightRate.pdf">Paper</a>
                <a href="https://brightvqa.github.io/BrightVQ/">Project</a>
                <a href="https://github.com/brightvqa/BrightVQ/">Code</a>
              </div>
            </div>
          </article>

          <article class="publication-card">
            <figure>
              <img src="images/chug.png" alt="CHUG HDR video dataset visualization">
            </figure>
            <div>
              <h3><a href="https://shreshthsaini.github.io/CHUG/">CHUG: Crowdsourced User-Generated HDR Video Quality Dataset</a></h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, N. Birkbeck, Y. Wang, B. Adsumilli, A. C. Bovik Â· IEEE ICIP 2025</p>
              <p>Presents the first large-scale crowdsourced HDR video quality dataset to unlock robust modeling and benchmarking for perceptual QA.</p>
              <div class="publication-links">
                <a href="https://shreshthsaini.github.io/CHUG/static/pdfs/chug.pdf">Paper</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/11084488">IEEE Xplore</a>
                <a href="https://shreshthsaini.github.io/CHUG/">Project</a>
                <a href="https://github.com/shreshthsaini/CHUG">Code</a>
              </div>
            </div>
          </article>

          <article class="publication-card">
            <figure>
              <img src="images/hidrovqa.png" alt="HIDRO-VQA contrastive learning visualization">
            </figure>
            <div>
              <h3><a href="https://openaccess.thecvf.com/content/WACV2024W/VAQ/papers/Saini_HIDRO-VQA_High_Dynamic_Range_Oracle_for_Video_Quality_Assessment_WACVW_2024_paper.pdf">Contrastive HDR-VQA: Deep Contrastive Representation Learning for HDR Video Quality</a></h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, A. Saha, A. C. Bovik Â· WACV 2024</p>
              <p>Employs contrastive representation learning to achieve state-of-the-art HDR video quality prediction with robust perceptual alignment.</p>
              <div class="publication-links">
                <a href="https://openaccess.thecvf.com/content/WACV2024W/VAQ/papers/Saini_HIDRO-VQA_High_Dynamic_Range_Oracle_for_Video_Quality_Assessment_WACVW_2024_paper.pdf">Paper</a>
                <a href="https://arxiv.org/abs/2311.11059">arXiv</a>
                <a href="https://github.com/shreshthsaini/HIDRO-VQA-DATA/tree/f00ee594ea3e5022d1d95427a1ef032c8d4c2782">Data</a>
                <a href="https://github.com/shreshthsaini/HIDRO-VQA">Code</a>
              </div>
            </div>
          </article>

          <article class="publication-card">
            <figure>
              <img src="images/ITM-DM.png" alt="Inverse tone mapping diffusion model concept">
            </figure>
            <div>
              <h3>ITM-DM: Using Diffusion Models for UGC Video Inverse Tone Mapping</h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, N. Birkbeck, Y. Wang, B. Adsumilli, A. C. Bovik Â· Ongoing work</p>
              <p>Explores diffusion-driven inverse tone mapping pipelines for user-generated HDR video, targeting higher fidelity playback.</p>
              <div class="publication-links">
                <span class="link-disabled">In development</span>
              </div>
            </div>
          </article>

          <article class="publication-card">
            <figure>
              <img src="images/prime-air.png" alt="Prime-EditBench benchmark illustration">
            </figure>
            <div>
              <h3>Prime-EditBench: A Real-World Benchmark for Diffusion-Based Editing</h3>
              <p class="publication-meta"><strong>Shreshth Saini</strong>, P. Korus, S. Jin Â· Preprint (Amazon)</p>
              <p>Introduces a benchmark for evaluating diffusion models on image and video editing tasks grounded in real-world assets.</p>
              <div class="publication-links">
                <span class="link-disabled">Preprint in preparation</span>
              </div>
            </div>
          </article>
        </div>

        <details class="details-block">
          <summary>Medical AI &amp; earlier research</summary>
          <div class="subsection-title">Medical AI publications</div>
          <div class="publication-grid">
            <article class="publication-card">
              <figure>
                <img src="images/mslae.png" alt="M2SLAe-Net retinal vessel segmentation visualization">
              </figure>
              <div>
                <h3>M2SLAe-Net: Multi-Scale Multi-Level Attention Embedded Network for Retinal Vessel Segmentation</h3>
                <p class="publication-meta"><strong>Shreshth Saini</strong>, G. Agrawal Â· IEEE ISBI 2021</p>
                <p>Proposes an attention-augmented encoderâ€“decoder for accurate retinal vessel segmentation across challenging clinical datasets.</p>
                <div class="publication-links">
                  <span class="link-disabled">Links coming soon</span>
                </div>
              </div>
            </article>

            <article class="publication-card">
              <figure>
                <img src="images/mslae.png" alt="(M)SLAe-Net retinal vessel segmentation diagram">
              </figure>
              <div>
                <h3><a href="https://ieeexplore.ieee.org/document/9565760">(M)SLAe-Net: Multi-Scale Multi-Level Attention Embedded Network for Retinal Vessel Segmentation</a></h3>
                <p class="publication-meta">S. Saini, G. Agrawal Â· IEEE ICHI 2021 (Oral)</p>
                <p>Captures intricate retinal vessel structure through multi-scale attention, enabling early disease detection.</p>
                <div class="publication-links">
                  <a href="https://ieeexplore.ieee.org/document/9565760">IEEE</a>
                  <a href="https://arxiv.org/pdf/2109.02084">arXiv</a>
                </div>
              </div>
            </article>

            <article class="publication-card">
              <figure>
                <img src="images/b-segnet.png" alt="B-SegNet skin lesion segmentation visualization">
              </figure>
              <div>
                <h3><a href="https://dl.acm.org/doi/10.1145/3450439.3451873">B-SegNet: Branched-SegMentor Network for Skin Lesion Segmentation</a></h3>
                <p class="publication-meta"><strong>Shreshth Saini</strong>, Y. S. Jeon, M. Feng Â· ACM CHIL 2021 (Oral)</p>
                <p>Employs a branched SegMentor architecture to capture both local and global lesion features for precision dermatology.</p>
                <div class="publication-links">
                  <a href="https://dl.acm.org/doi/10.1145/3450439.3451873">ACM</a>
                </div>
              </div>
            </article>

            <article class="publication-card">
              <figure>
                <img src="images/detector-segnet.png" alt="Detector-SegMentor skin lesion model visualization">
              </figure>
              <div>
                <h3><a href="https://link.springer.com/chapter/10.1007/978-981-15-8697-2_55">Detector-SegMentor Network for Skin Lesion Localization and Segmentation</a></h3>
                <p class="publication-meta"><strong>Shreshth Saini</strong>, D. Gupta, A. K. Tiwari Â· NCVPRIPG 2019 (Oral)</p>
                <p>Combines detection and segmentation tasks to deliver comprehensive skin lesion analysis in unconstrained settings.</p>
                <div class="publication-links">
                  <a href="https://arxiv.org/pdf/2005.06550">arXiv</a>
                </div>
              </div>
            </article>
          </div>

          <div class="subsection-title">Journal articles</div>
          <div class="publication-grid">
            <article class="publication-card">
              <figure>
                <img src="images/pixel-seg.png" alt="PixISegNet iris segmentation visualization">
              </figure>
              <div>
                <h3><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-bmt.2019.0025">PixISegNet: Pixel-Level Iris Segmentation Using Stacked Hourglass Networks</a></h3>
                <p class="publication-meta">R. R. Jha, G. Jaswal, <strong>Shreshth Saini</strong>, D. Gupta, A. Nigam Â· IET Biometrics 2019</p>
                <p>Delivers precise iris segmentation via a stacked hourglass encoderâ€“decoder tailored for biometric security systems.</p>
                <div class="publication-links">
                  <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-bmt.2019.0025">IET</a>
                </div>
              </div>
            </article>
          </div>

          <div class="subsection-title">Book chapters</div>
          <div class="publication-grid">
            <article class="publication-card">
              <figure>
                <img src="images/chapter.png" alt="Book chapter on iris segmentation">
              </figure>
              <div>
                <h3><a href="https://unidel.edu.ng/focelibrary/books/AI%20and%20Deep%20Learning%20in%20Biometric%20Security%20Trends,%20Potential,%20and%20Challenges%20by%20Gaurav%20Jaswal,%20Vivek%20Kanhangad,%20Raghavendra%20Ramachandra%20(z-lib.org)%20(2).pdf">Iris Segmentation in the Wild Using Encoderâ€“Decoder Based Deep Learning Techniques</a></h3>
                <p class="publication-meta"><strong>Shreshth Saini</strong>, D. Gupta, R. R. Jha, G. Jaswal, A. Nigam Â· AI and Deep Learning in Biometric Security, 2020</p>
                <p>Explores deep encoderâ€“decoder techniques for iris segmentation under unconstrained, real-world conditions.</p>
                <div class="publication-links">
                  <a href="https://www.taylorfrancis.com/chapters/edit/10.1201/9781003003489-12/iris-segmentation-wild-using-encoder-decoder-based-deep-learning-techniques-shreshth-saini-divij-gupta-ranjeet-ranjan-jha-gaurav-jaswal-aditya-nigam">Taylor &amp; Francis</a>
                </div>
              </div>
            </article>
          </div>
        </details>
      </section>

      <section id="projects" class="panel">
        <div class="section-heading">
          <h2>Selected projects</h2>
          <p class="section-subtitle">Open-source explorations that translate research into deployable systems.</p>
        </div>
        <div class="card-grid">
          <article class="card">
            <img src="images/inverse-cse.png" alt="General inverse problems coursework illustration">
            <h3><a href="https://github.com/shreshthsaini/Inverse_Problems_CSE_393P">Problems on General Inverse and Solution (CSE 393P)</a></h3>
            <p>Collection of problems and solutions for inverse problems covering implementations and analyses of reconstruction techniques.</p>
            <div class="card-links">
              <a href="https://github.com/shreshthsaini/Inverse_Problems_CSE_393P">GitHub</a>
            </div>
          </article>

          <article class="card">
            <img src="images/srdm.png" alt="Super-resolution diffusion model illustration">
            <h3><a href="https://github.com/shreshthsaini/SR-DDPM">An Efficient Approach to Super-Resolution with Fine-Tuning Diffusion Models</a></h3>
            <p>Implements an efficient SR3 diffusion pipeline to improve generalization and reduce compute for image super-resolution.</p>
            <div class="card-links">
              <a href="https://github.com/shreshthsaini/SR-DDPM">GitHub</a>
              <a href="https://github.com/shreshthsaini/SR-DDPM/blob/main/An%20Efficient%20Approach%20to%20Super-Resolution%20with%20Fine-Tuning%20Diffusion%20Models.pdf">PDF</a>
            </div>
          </article>

          <article class="card">
            <img src="images/zero-da.png" alt="Zero-DA diffusion animation illustration">
            <h3><a href="https://github.com/shreshthsaini/Zero-DA">Zero-DA: Zero-shot Diffusion Model for Video Animation</a></h3>
            <p>Adapts image generation models to video with hierarchical cross-frame constraints that preserve temporal uniformity.</p>
            <div class="card-links">
              <a href="https://github.com/shreshthsaini/Zero-DA">GitHub</a>
              <a href="https://github.com/shreshthsaini/Zero-DA/blob/master/paper/Zero-DA.pdf">PDF</a>
            </div>
          </article>

          <article class="card">
            <img src="images/recividism.png" alt="Recidivism fairness project visualization">
            <h3><a href="https://github.com/shreshthsaini/UBR-UnBiased-and-Robust-Recidivism-Prediction?tab=readme-ov-file#ubr-unbiased-and-robust-recidivism-prediction">UBR: Unbiased &amp; Robust Recidivism Prediction</a></h3>
            <p>Mitigates gender and racial bias in recidivism scoring using fairness-aware machine learning pipelines.</p>
            <div class="card-links">
              <a href="https://github.com/shreshthsaini/UBR-UnBiased-and-Robust-Recidivism-Prediction?tab=readme-ov-file#ubr-unbiased-and-robust-recidivism-prediction">GitHub</a>
              <a href="https://github.com/shreshthsaini/UBR-UnBiased-and-Robust-Recidivism-Prediction/blob/master/Recidivism.pdf">PDF</a>
            </div>
          </article>

          <article class="card">
            <img src="images/flowless.png" alt="Optical flow-less interpolation illustration">
            <h3><a href="https://github.com/shreshthsaini/Flow-Less-VFI">Optical Flow-less Video Frame Interpolation</a></h3>
            <p>Replaces explicit motion estimation with transformer-based mutual self-attention for long-range video interpolation.</p>
            <div class="card-links">
              <a href="https://github.com/shreshthsaini/Flow-Less-VFI">GitHub</a>
              <a href="https://github.com/shreshthsaini/Flow-Less-VFI/blob/main/Digital_Video___Report.pdf">PDF</a>
            </div>
          </article>
        </div>
      </section>

      <section id="contact" class="panel contact-panel">
        <div class="section-heading">
          <h2>Letâ€™s collaborate</h2>
          <p class="section-subtitle">I am always excited to partner on projects that push the boundaries of generative modeling, HDR media, and perceptual intelligence.</p>
        </div>
        <div class="contact-actions">
          <a href="mailto:saini.2@utexas.edu">Email</a>
          <a href="https://scholar.google.co.in/citations?user=OpZ-5K4AAAAJ&hl=en">Google Scholar</a>
          <a href="https://www.linkedin.com/in/shreshthfeel/">LinkedIn</a>
          <a href="https://github.com/shreshthsaini">GitHub</a>
          <a href="https://x.com/shreshthsaini">X</a>
        </div>
        <div class="map-card">
          <img src="//www.clustrmaps.com/map_v2.png?d=jgy4unsMrAZelCO909ghCVFsfJl5bSYJhOWARYTbZ-o&cl=ffffff" alt="Visitor map">
        </div>
      </section>
    </main>

    <footer>
      Â© 2025 Shreshth Saini. Crafted with inspiration from Thinking Machinesâ€™ design language.
    </footer>
  </div>
</body>
</html>
