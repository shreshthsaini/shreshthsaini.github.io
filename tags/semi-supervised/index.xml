<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>semi-supervised | meliorate</title>
    <link>https://shreshthsaini.github.io/tags/semi-supervised/</link>
      <atom:link href="https://shreshthsaini.github.io/tags/semi-supervised/index.xml" rel="self" type="application/rss+xml" />
    <description>semi-supervised</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020</copyright><lastBuildDate>Mon, 31 Aug 2020 20:06:52 +0530</lastBuildDate>
    <image>
      <url>https://shreshthsaini.github.io/img/icon-192.png</url>
      <title>semi-supervised</title>
      <link>https://shreshthsaini.github.io/tags/semi-supervised/</link>
    </image>
    
    <item>
      <title>Variational AutoEncoder</title>
      <link>https://shreshthsaini.github.io/post/variational-autoencoder/</link>
      <pubDate>Mon, 31 Aug 2020 20:06:52 +0530</pubDate>
      <guid>https://shreshthsaini.github.io/post/variational-autoencoder/</guid>
      <description>

&lt;p&gt;In the last few years deep learning practictioners have developed a huge interest in deep generative models. Credit for this can be given to availability of huge datasets, computational capacity, efficient optimization methods and well-designed networks. With all these, generative model are now able to produce realistic contents be it images, texts or voice/music. In this post I have tried to present my understandig of one such generative approach : Variational Auto-Encoder.&lt;/p&gt;

&lt;h3 id=&#34;auto-encoders&#34;&gt;Auto-Encoders&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Before we actually dive into the Variational autoencoders, lets have a clarity about what are autoencoders.&lt;/em&gt;&lt;/strong&gt;
Autoencoder are used for data compression and other derivative tasks such as segmentation, noise reduction, feature manipulation in image, etc.. Autoencoder consists of two part namely &lt;strong&gt;&lt;em&gt;Encoder&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Decoder&lt;/em&gt;&lt;/strong&gt;. Encoder(E) takes the input(X) and produces corresponding latent space representation(E(X)), output of encoder is then fed to decoder(D) which gives us our final output(D(E(X))). Autoencoder are trained supervisingly (X,Y). With convolutional autoencoder we can extract high dimensional representation of our data from latent space. For data compression latent is have fewer features (mainly with fully connected neural network design).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;autoencoder.jpg &#39;Fig. 1: Convolutional Autoencoder.[[1]](#1&#34; alt=&#34;autoencoder-sample.jpg&#34; /&gt;&amp;rsquo;)
&lt;em&gt;Fig. 1: Convolutional Autoencoder.&lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;variational-auto-encoders-vaes&#34;&gt;Variational Auto-Encoders(VAEs)&lt;/h3&gt;

&lt;p&gt;Having understood the basics of autoencoder and its structure now we can move towards the real thing. It it to be notes that latent space representations from simple autoencoder can not be used in generation task as it gives the a rather unrealistic output. Reason being distribution of latent space is often not continuous. Variational autonencoders introduce additional layers at bottleneck of the network to extract the probablistic distribution of the latent space (mean and variance), in additional a KL-Divergence loss &lt;a href=&#34;#2&#34;&gt;[2]&lt;/a&gt; is also used to bring the data distribution close up for interpolation inbetween the classes (generation). In general results of VAE are blurry, novel loss functions such as generative loss &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt; or perceptual loss &lt;a href=&#34;#4&#34;&gt;[4]&lt;/a&gt; can be used to remove the blurriness.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vae.jpg&#34; alt=&#34;vae-sample.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig. 2: Convolutional Variational Autoencoder. Low dimensional representation feature is sampled from learned distribution at bottlecneck. Unit gaussian distribution depicts convergence of learned distribution towards normal distribution.&lt;a href=&#34;#5&#34;&gt;[5]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;REFERENCES:&lt;/h3&gt;

&lt;p&gt;&lt;a id=&#34;1&#34;&gt;[1]&lt;/a&gt;
&lt;a href=&#34;https://www.cs.umd.edu/sites/default/files/scholarly_papers/Larrue,%20Tara_1801.pdf&#34; target=&#34;_blank&#34;&gt;Tara Larrue and Xiaoxu Meng and Chang-Mu Han.
Denoising Videos with Convolutional Autoencoders A Comparison of Autoencoder Architectures, 2018.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;2&#34;&gt;[2]&lt;/a&gt;
&lt;a href=&#34;https://doi.org/10.1007/978-3-642-04898-2_327&#34; target=&#34;_blank&#34;&gt;Joyce, James M., Lovric, Miodrag.
Kullback-Leibler Divergence. International Encyclopedia of Statistical Science
Springer Berlin Heidelberg. 720&amp;ndash;722, 978-3-642-04898-2. 2011
DOI:10.&lt;sup&gt;1007&lt;/sup&gt;&amp;frasl;&lt;sub&gt;978&lt;/sub&gt;-3-642-04898-2_327&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;3&#34;&gt;[3]&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/1512.09300&#34; target=&#34;_blank&#34;&gt;Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther.
Autoencoding beyond pixels using a learned similarity metric. 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;4&#34;&gt;[4]&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/1610.00291&#34; target=&#34;_blank&#34;&gt;Xianxu Hou, Linlin Shen, Ke Sun, Guoping Qiu.
Deep Feature Consistent Variational Autoencoder. 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;5&#34;&gt;[5]&lt;/a&gt;
&lt;a href=&#34;https://iq.opengenus.org/types-of-autoencoder/&#34; target=&#34;_blank&#34;&gt;Abhinav Prakash.
Different types of Autoencoders. opengenus.org.
University of Massachusetts, Amherst.&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
