<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Shreshth Saini | AI Researcher</title>
  <meta name="author" content="Shreshth Saini">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Shreshth Saini - PhD Candidate at UT Austin researching Generative AI, Diffusion Models, and Video Quality Assessment">
  
  <link rel="icon" type="image/jpg" href="images/UT_profile.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Pro:wght@400;500;600&amp;display=swap" rel="stylesheet">
  
  <style>
    :root {
      --text-primary: #0f172a;
      --text-secondary: #475569;
      --text-muted: #64748b;
      --accent: #2563eb;
      --accent-light: #3b82f6;
      --accent-hover: #1d4ed8;
      --bg: #ffffff;
      --bg-subtle: #f8fafc;
      --border: #e2e8f0;
      --border-light: #f1f5f9;
      --success: #059669;
      --gradient-start: #2563eb;
      --gradient-end: #7c3aed;
    }
    
    * { margin: 0; padding: 0; box-sizing: border-box; }
    html { scroll-behavior: smooth; }
    
    body {
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
      font-size: 15px;
      line-height: 1.7;
      color: var(--text-secondary);
      background: var(--bg);
      -webkit-font-smoothing: antialiased;
    }
    
    .bg-gradient {
      position: fixed;
      top: 0; left: 0; right: 0;
      height: 400px;
      background: linear-gradient(180deg, rgba(37, 99, 235, 0.03) 0%, rgba(255, 255, 255, 0) 100%);
      pointer-events: none;
      z-index: -1;
    }
    
    nav {
      position: sticky;
      top: 0;
      background: rgba(255, 255, 255, 0.85);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      border-bottom: 1px solid var(--border-light);
      z-index: 100;
      padding: 0 20px;
    }
    
    .nav-container {
      max-width: 1000px;
      margin: 0 auto;
      display: flex;
      justify-content: space-between;
      align-items: center;
      height: 60px;
    }
    
    .nav-logo {
      font-weight: 600;
      font-size: 16px;
      color: var(--text-primary);
      text-decoration: none;
      letter-spacing: -0.3px;
    }
    
    .nav-links { display: flex; gap: 8px; }
    
    .nav-links a {
      padding: 8px 14px;
      color: var(--text-secondary);
      text-decoration: none;
      font-size: 13px;
      font-weight: 500;
      border-radius: 6px;
      transition: all 0.15s ease;
    }
    
    .nav-links a:hover { color: var(--accent); background: var(--bg-subtle); }
    .nav-links a.active { color: var(--accent); background: rgba(37, 99, 235, 0.08); }
    
    .container { max-width: 900px; margin: 0 auto; padding: 0 24px; }
    
    .hero {
      padding: 80px 0 60px;
      display: flex;
      gap: 48px;
      align-items: flex-start;
    }
    
    .hero-content { flex: 1; }
    .hero-image { flex-shrink: 0; }
    
    .hero-image img {
      width: 180px;
      height: 180px;
      object-fit: cover;
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .hero-image img:hover {
      transform: translateY(-4px);
      box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
    }
    
    h1 {
      font-family: 'Crimson Pro', Georgia, serif;
      font-size: 42px;
      font-weight: 500;
      color: var(--text-primary);
      letter-spacing: -1px;
      margin-bottom: 16px;
      line-height: 1.2;
    }
    
    .bio { font-size: 16px; line-height: 1.8; margin-bottom: 20px; }
    .bio strong { color: var(--text-primary); font-weight: 600; }
    .bio a { color: var(--accent); text-decoration: none; font-weight: 500; }
    .bio a:hover { text-decoration: underline; }
    
    .company-tag { display: inline-flex; align-items: center; gap: 4px; font-weight: 600; }
    .company-tag.youtube { color: #FF0000; }
    .company-tag.google { color: #4285F4; }
    
    .opportunity-banner {
      background: linear-gradient(135deg, rgba(5, 150, 105, 0.08) 0%, rgba(37, 99, 235, 0.08) 100%);
      border: 1px solid rgba(5, 150, 105, 0.2);
      border-radius: 10px;
      padding: 16px 20px;
      margin: 24px 0;
      display: flex;
      align-items: flex-start;
      gap: 12px;
    }
    
    .opportunity-icon {
      width: 24px; height: 24px;
      background: var(--success);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      flex-shrink: 0;
      margin-top: 2px;
    }
    
    .opportunity-icon svg { width: 14px; height: 14px; color: white; }
    .opportunity-content { flex: 1; }
    .opportunity-title { font-weight: 600; color: var(--success); font-size: 14px; margin-bottom: 4px; }
    .opportunity-text { font-size: 14px; color: var(--text-secondary); line-height: 1.6; }
    
    .social-links { display: flex; flex-wrap: wrap; gap: 16px; margin-top: 20px; }
    .social-links a {
      color: var(--text-muted);
      text-decoration: none;
      font-size: 13px;
      font-weight: 500;
      display: flex;
      align-items: center;
      gap: 6px;
      transition: color 0.15s ease;
    }
    .social-links a:hover { color: var(--accent); }
    
    .section { padding: 48px 0; border-top: 1px solid var(--border-light); }
    .section-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 28px; }
    
    h2 {
      font-family: 'Crimson Pro', Georgia, serif;
      font-size: 26px;
      font-weight: 500;
      color: var(--text-primary);
      letter-spacing: -0.5px;
    }
    
    .updates-list { list-style: none; }
    .updates-list li {
      padding: 12px 0;
      border-bottom: 1px solid var(--border-light);
      display: flex;
      gap: 16px;
      font-size: 14px;
    }
    .updates-list li:last-child { border-bottom: none; }
    .update-date { flex-shrink: 0; width: 90px; font-weight: 600; color: var(--text-muted); font-size: 13px; }
    .update-content { flex: 1; }
    .update-content strong { color: var(--text-primary); }
    .update-content a { color: var(--accent); text-decoration: none; }
    
    .research-category { margin-bottom: 48px; }
    .category-tag {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 6px 12px;
      background: var(--bg-subtle);
      border: 1px solid var(--border);
      border-radius: 6px;
      font-size: 12px;
      font-weight: 600;
      color: var(--text-muted);
      text-transform: uppercase;
      letter-spacing: 0.5px;
      margin-bottom: 20px;
    }
    
    .category-discrete { border-color: #8b5cf6; color: #7c3aed; background: rgba(139, 92, 246, 0.08); }
    .category-continuous { border-color: #06b6d4; color: #0891b2; background: rgba(6, 182, 212, 0.08); }
    .category-mllm { border-color: #f59e0b; color: #d97706; background: rgba(245, 158, 11, 0.08); }
    .category-vqa { border-color: #10b981; color: #059669; background: rgba(16, 185, 129, 0.08); }
    .category-other { border-color: #64748b; color: #475569; background: rgba(100, 116, 139, 0.08); }
    
    .paper-grid { display: flex; flex-direction: column; gap: 24px; }
    .paper-card {
      display: flex;
      gap: 20px;
      padding: 20px;
      background: var(--bg);
      border: 1px solid var(--border-light);
      border-radius: 12px;
      transition: all 0.2s ease;
    }
    .paper-card:hover {
      border-color: var(--border);
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.04);
      transform: translateY(-2px);
    }
    
    .paper-image {
      flex-shrink: 0;
      width: 140px;
      height: 140px;
      overflow: hidden;
      border-radius: 8px;
      background: var(--bg-subtle);
    }
    .paper-image img { width: 100%; height: 100%; object-fit: cover; }
    
    .paper-content { flex: 1; min-width: 0; }
    .paper-title { font-weight: 600; font-size: 15px; color: var(--text-primary); margin-bottom: 6px; line-height: 1.4; }
    .paper-title a { color: inherit; text-decoration: none; }
    .paper-title a:hover { color: var(--accent); }
    .paper-authors { font-size: 13px; color: var(--text-muted); margin-bottom: 6px; }
    .paper-authors strong { color: var(--text-primary); }
    .paper-venue { font-size: 13px; font-weight: 500; color: var(--accent); margin-bottom: 10px; }
    .paper-abstract {
      font-size: 13px;
      color: var(--text-secondary);
      line-height: 1.6;
      margin-bottom: 12px;
      display: -webkit-box;
      -webkit-line-clamp: 3;
      -webkit-box-orient: vertical;
      overflow: hidden;
    }
    .paper-links { display: flex; flex-wrap: wrap; gap: 8px; }
    .paper-link {
      display: inline-flex;
      align-items: center;
      gap: 4px;
      padding: 5px 10px;
      font-size: 12px;
      font-weight: 500;
      color: var(--text-muted);
      background: var(--bg-subtle);
      border: 1px solid var(--border-light);
      border-radius: 5px;
      text-decoration: none;
      transition: all 0.15s ease;
    }
    .paper-link:hover { color: var(--accent); border-color: var(--accent); background: rgba(37, 99, 235, 0.04); }
    
    .project-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; }
    .project-card {
      padding: 20px;
      background: var(--bg);
      border: 1px solid var(--border-light);
      border-radius: 12px;
      transition: all 0.2s ease;
    }
    .project-card:hover { border-color: var(--border); box-shadow: 0 4px 20px rgba(0, 0, 0, 0.04); }
    .project-title { font-weight: 600; font-size: 14px; color: var(--text-primary); margin-bottom: 8px; }
    .project-title a { color: inherit; text-decoration: none; }
    .project-title a:hover { color: var(--accent); }
    .project-desc { font-size: 13px; color: var(--text-secondary); line-height: 1.6; }
    
    .experience-list { list-style: none; }
    .experience-list li { padding: 14px 0; border-bottom: 1px solid var(--border-light); font-size: 14px; }
    .experience-list li:last-child { border-bottom: none; }
    .experience-list strong { color: var(--text-primary); }
    .experience-list a { color: var(--accent); text-decoration: none; }
    .exp-date { color: var(--text-muted); font-weight: 500; }
    
    .services-text { font-size: 14px; line-height: 1.8; }
    .services-text strong { color: var(--text-primary); }
    
    .btn {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 8px 16px;
      font-size: 13px;
      font-weight: 500;
      color: var(--text-secondary);
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      cursor: pointer;
      transition: all 0.15s ease;
      text-decoration: none;
    }
    .btn:hover { color: var(--accent); border-color: var(--accent); }
    
    footer { padding: 48px 0; border-top: 1px solid var(--border-light); text-align: center; }
    .footer-map { margin-bottom: 24px; }
    .footer-text { font-size: 13px; color: var(--text-muted); }
    
    .particles {
      position: fixed;
      top: 0; left: 0;
      width: 100%; height: 100%;
      pointer-events: none;
      z-index: -1;
      overflow: hidden;
    }
    .particle {
      position: absolute;
      width: 4px; height: 4px;
      background: var(--accent);
      border-radius: 50%;
      opacity: 0.15;
      animation: float 20s infinite ease-in-out;
    }
    @keyframes float {
      0%, 100% { transform: translateY(0) translateX(0); }
      25% { transform: translateY(-30px) translateX(20px); }
      50% { transform: translateY(-10px) translateX(-20px); }
      75% { transform: translateY(-40px) translateX(10px); }
    }
    
    .scroll-indicator {
      position: fixed;
      top: 60px; left: 0;
      height: 2px;
      background: linear-gradient(90deg, var(--gradient-start), var(--gradient-end));
      z-index: 101;
      transition: width 0.1s ease;
    }
    
    @media (max-width: 768px) {
      .hero { flex-direction: column-reverse; text-align: center; gap: 32px; padding: 48px 0 40px; }
      .hero-image { margin: 0 auto; }
      h1 { font-size: 32px; }
      .social-links { justify-content: center; }
      .project-grid { grid-template-columns: 1fr; }
      .paper-card { flex-direction: column; }
      .paper-image { width: 100%; height: 180px; }
      .nav-links a { padding: 6px 10px; font-size: 12px; }
    }
  </style>
</head>

<body>
  <div class="bg-gradient"></div>
  <div class="particles" id="particles"></div>
  <div class="scroll-indicator" id="scrollIndicator"></div>
  
  <nav>
    <div class="nav-container">
      <a href="index.html" class="nav-logo">Shreshth Saini</a>
      <div class="nav-links">
        <a href="#about" class="active">About</a>
        <a href="#research">Research</a>
        <a href="#projects">Projects</a>
        <a href="#experience">Experience</a>
        <a href="blogs.html">Blog</a>
        <a href="data/CV_Shreshth_Saini_2026.pdf">CV</a>
      </div>
    </div>
  </nav>

  <div class="container">
    <section class="hero" id="about">
      <div class="hero-content">
        <h1>Shreshth Saini</h1>
        <p class="bio">
          I am a third-year PhD candidate at the <a href="https://live.ece.utexas.edu/">Laboratory of Image and Video Engineering</a> at <a href="https://www.utexas.edu/">UT Austin</a>, advised by <a href="https://www.ece.utexas.edu/people/faculty/alan-bovik">Prof. Alan C. Bovik</a>.
        </p>
        <p class="bio">
          My research focuses on <strong>Theoretical Foundations of Generative Models</strong> (Flows, Diffusion, MLLMs) and their applications in efficient sampling, image/video quality assessment, editing, and inverse problems. I collaborate with <span class="company-tag youtube">YouTube</span>/<span class="company-tag google">Google</span> Media Algorithms team.
        </p>
        
        <div class="opportunity-banner">
          <div class="opportunity-icon">
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>
          </div>
          <div class="opportunity-content">
            <div class="opportunity-title">Open to Opportunities</div>
            <div class="opportunity-text">Actively seeking full-time <strong>Research Scientist</strong> positions starting <strong>2026</strong>. Interested in generative AI, video understanding, and quality assessment research.</div>
          </div>
        </div>
        
        <p class="bio">
          Previously, I worked as a Research Engineer-AI at Arkray, Inc. and ML Engineer at BioMind AI, developing scalable AI solutions for medical image analysis. I received my B.Tech in Electrical Engineering from <a href="https://www.iitj.ac.in/">IIT Jodhpur</a>.
        </p>
        
        <div class="social-links">
          <a href="mailto:saini.2@utexas.edu">Email</a>
          <a href="https://scholar.google.co.in/citations?user=OpZ-5K4AAAAJ&amp;hl=en">Scholar</a>
          <a href="https://www.linkedin.com/in/shreshthfeel/">LinkedIn</a>
          <a href="https://x.com/shreshthsaini">X / Twitter</a>
          <a href="https://github.com/shreshthsaini">GitHub</a>
          <a href="data/CV_Shreshth_Saini_2026.pdf">CV</a>
        </div>
      </div>
      <div class="hero-image">
        <img src="images/shreshth-portrait.jpeg" alt="Shreshth Saini">
      </div>
    </section>

    <section class="section" id="updates">
      <div class="section-header">
        <h2>Recent Updates</h2>
      </div>
      <ul class="updates-list" id="updatesList">
        <li><span class="update-date">Jan 2026</span><span class="update-content">Honored to serve as a reviewer for <strong>ICML 2026</strong> and <strong>ECCV 2026</strong>.</span></li>
        <li><span class="update-date">Sept 2025</span><span class="update-content"><strong>Rectified CFG++</strong> accepted to <strong>NeurIPS 2025</strong>.</span></li>
        <li><span class="update-date">Jun - Oct 2025</span><span class="update-content">Completed <strong>Google Research</strong> internship as Student Researcher in the LUMA team.</span></li>
        <li><span class="update-date">May 2025</span><span class="update-content">Papers accepted in <strong>IEEE ICIP 2025</strong>, <strong>ICML 2025</strong>, and <strong>GenCV Workshop @ CVPR 2025</strong>.</span></li>
        <li><span class="update-date">Mar 2025</span><span class="update-content">Appointed as <strong>Assistant Director</strong> of <a href="https://live.ece.utexas.edu/">LIVE at UT Austin</a>.</span></li>
        <li id="moreUpdates" style="display: none;"><span class="update-date">Jun 2024</span><span class="update-content">Joined <strong>Amazon</strong> as Applied Scientist-II Intern in the Perception team.</span></li>
      </ul>
      <button class="btn" onclick="toggleUpdates()" id="updatesBtn" style="margin-top: 16px;">Show More</button>
    </section>

    <section class="section" id="research">
      <div class="section-header">
        <h2>Research</h2>
      </div>
      
      <div class="research-category">
        <div class="category-tag category-discrete">Discrete Diffusion Models</div>
        <div class="paper-grid">
          <div class="paper-card">
            <div class="paper-image"><img src="images/mdm-sampling.png" alt="MDM Sampling"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="#">Efficient Sampling Strategies for Masked Discrete Diffusion Models</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, et al.</div>
              <div class="paper-venue">Under Review</div>
              <div class="paper-abstract">Novel efficient sampling strategies for masked discrete diffusion models, enabling faster generation while maintaining quality through adaptive masking and parallel decoding techniques.</div>
              <div class="paper-links"><a href="#" class="paper-link">PDF</a><a href="#" class="paper-link">ArXiv</a><a href="#" class="paper-link">Code</a></div>
            </div>
          </div>
        </div>
      </div>

      <div class="research-category">
        <div class="category-tag category-continuous">Continuous Diffusion &amp; Flow Models</div>
        <div class="paper-grid">
          <div class="paper-card">
            <div class="paper-image"><img src="images/rect-cfg.png" alt="Rectified CFG++"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="https://shreshthsaini.github.io/Rectified-CFGpp/">Rectified-CFG++ for Flow-based Models</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, S Gupta, AC Bovik</div>
              <div class="paper-venue">NeurIPS 2025</div>
              <div class="paper-abstract">Rectified CFG++ enhances conditional image generation with Rectified Flow models by adaptively correcting the latent trajectory, improving visual coherence and alignment with text prompts.</div>
              <div class="paper-links"><a href="https://shreshthsaini.github.io/Rectified-CFGpp/data/pdfs/Rect_CFGpp_Neurips-compressed-names.pdf" class="paper-link">PDF</a><a href="https://neurips.cc/virtual/2025/poster/118333" class="paper-link">NeurIPS</a><a href="https://shreshthsaini.github.io/Rectified-CFGpp/" class="paper-link">Page</a><a href="https://github.com/shreshthsaini/Rectified-CFGpp" class="paper-link">Code</a></div>
            </div>
          </div>
          <div class="paper-card">
            <div class="paper-image"><img src="images/pcdm.png" alt="LGDM"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="data/camera_ready_LGDM.pdf">LGDM: Latent Guidance in Diffusion Models for Perceptual Evaluations</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, R Liao, Y Ye, AC Bovik</div>
              <div class="paper-venue">ICML 2025</div>
              <div class="paper-abstract">Investigates the exploitation of diffusion model priors to achieve perceptual consistency in image quality assessment.</div>
              <div class="paper-links"><a href="https://openreview.net/pdf?id=lIdcR7vU76" class="paper-link">PDF</a><a href="https://arxiv.org/abs/2506.00327" class="paper-link">ArXiv</a></div>
            </div>
          </div>
          <div class="paper-card">
            <div class="paper-image"><img src="images/flowless.png" alt="LumaFlux"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="#">LumaFlux: Efficient HDR Image Reconstruction via Flow-based Inverse Tone Mapping</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, D Tian, Y Ren, et al.</div>
              <div class="paper-venue">GenCV Workshop @ CVPR 2025</div>
              <div class="paper-abstract">LumaFlux presents an efficient flow-based approach to HDR image reconstruction through inverse tone mapping using rectified flow models.</div>
              <div class="paper-links"><a href="#" class="paper-link">PDF</a><a href="#" class="paper-link">ArXiv</a></div>
            </div>
          </div>
        </div>
      </div>

      <div class="research-category">
        <div class="category-tag category-mllm">Multimodal LLMs</div>
        <div class="paper-grid">
          <div class="paper-card">
            <div class="paper-image"><img src="images/ugc-hdr-mllm.png" alt="UGC-HDR MLLM"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="#">Seeing Beyond 8-bits: Reasoning Through Perceptual Quality for UGC-HDR Videos</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, N Birkbeck, Y Wang, B Adsumilli, AC Bovik</div>
              <div class="paper-venue">Under Review: ICLR 2026</div>
              <div class="paper-abstract">We propose a 40K UGC-HDR subjective video quality database and train an HDR-native reasoning MLLM for perceptual video quality assessment.</div>
              <div class="paper-links"><a href="#" class="paper-link">PDF</a><a href="#" class="paper-link">ArXiv</a></div>
            </div>
          </div>
        </div>
      </div>

      <div class="research-category">
        <div class="category-tag category-vqa">Video &amp; Image Quality Assessment</div>
        <div class="paper-grid">
          <div class="paper-card">
            <div class="paper-image"><img src="images/brightrate.png" alt="BrightRate"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="https://brightvqa.github.io/BrightVQ/">BrightRate: Quality Assessment for User-Generated HDR Videos</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, B Chen, N Birkbeck, Y Wang, B Adsumilli, AC Bovik</div>
              <div class="paper-venue">Under Review</div>
              <div class="paper-abstract">BrightRate is designed for quality assessment in user-generated HDR videos, focusing on unique challenges like varying content and capture conditions.</div>
              <div class="paper-links"><a href="https://brightvqa.github.io/BrightVQ/static/pdfs/BrightRate.pdf" class="paper-link">PDF</a><a href="https://brightvqa.github.io/BrightVQ/" class="paper-link">Page</a><a href="https://github.com/brightvqa/BrightVQ/" class="paper-link">Code</a></div>
            </div>
          </div>
          <div class="paper-card">
            <div class="paper-image"><img src="images/chug.png" alt="CHUG"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="https://shreshthsaini.github.io/CHUG/">CHUG: Crowdsourced User-Generated HDR Video Quality Dataset</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, N Birkbeck, Y Wang, B Adsumilli, AC Bovik</div>
              <div class="paper-venue">IEEE ICIP 2025</div>
              <div class="paper-abstract">CHUG is a crowdsourced dataset for HDR video quality, addressing the need for diverse, real-world content to develop accurate quality assessment models.</div>
              <div class="paper-links"><a href="https://shreshthsaini.github.io/CHUG/static/pdfs/chug.pdf" class="paper-link">PDF</a><a href="https://ieeexplore.ieee.org/abstract/document/11084488" class="paper-link">IEEE</a><a href="https://shreshthsaini.github.io/CHUG/" class="paper-link">Page</a><a href="https://github.com/shreshthsaini/CHUG" class="paper-link">Code</a></div>
            </div>
          </div>
          <div class="paper-card">
            <div class="paper-image"><img src="images/hidrovqa.png" alt="HIDRO-VQA"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="https://openaccess.thecvf.com/content/WACV2024W/VAQ/papers/Saini_HIDRO-VQA_High_Dynamic_Range_Oracle_for_Video_Quality_Assessment_WACVW_2024_paper.pdf">Contrastive HDR-VQA: Deep Contrastive Representation Learning for HDR Video Quality Assessment</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, A Saha, AC Bovik</div>
              <div class="paper-venue">WACV 2024</div>
              <div class="paper-abstract">Introduces deep contrastive representation learning for high dynamic range video quality assessment, achieving state-of-the-art performance.</div>
              <div class="paper-links"><a href="https://arxiv.org/abs/2311.11059" class="paper-link">ArXiv</a><a href="https://github.com/shreshthsaini/HIDRO-VQA-DATA/" class="paper-link">Data</a><a href="https://github.com/shreshthsaini/HIDRO-VQA" class="paper-link">Code</a></div>
            </div>
          </div>
        </div>
      </div>

      <div class="research-category">
        <div class="category-tag category-other">Other Research (Medical AI, Biometrics)</div>
        <div class="paper-grid">
          <div class="paper-card">
            <div class="paper-image"><img src="images/mslae.png" alt="M2SLAe-Net"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="https://ieeexplore.ieee.org/document/9565760">(M)SLAe-Net: Multi-Scale Multi-Level Attention Embedded Network for Retinal Vessel Segmentation</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, G Agrawal</div>
              <div class="paper-venue">IEEE ISBI 2021 &amp; IEEE ICHI 2021</div>
              <div class="paper-abstract">Multi-scale multi-level attention embedded network for improved retinal vessel segmentation.</div>
              <div class="paper-links"><a href="https://ieeexplore.ieee.org/document/9565760" class="paper-link">PDF</a><a href="https://arxiv.org/pdf/2109.02084" class="paper-link">ArXiv</a></div>
            </div>
          </div>
          <div class="paper-card">
            <div class="paper-image"><img src="images/b-segnet.png" alt="B-SegNet"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="https://dl.acm.org/doi/10.1145/3450439.3451873">B-SegNet: Branched-SegMentor Network for Skin Lesion Segmentation</a></div>
              <div class="paper-authors"><strong>S Saini</strong>, YS Jeon, M Feng</div>
              <div class="paper-venue">ACM CHIL 2021 (Oral)</div>
              <div class="paper-abstract">Branched SegMentor network for accurate skin lesion segmentation.</div>
              <div class="paper-links"><a href="https://dl.acm.org/doi/10.1145/3450439.3451873" class="paper-link">PDF</a></div>
            </div>
          </div>
          <div class="paper-card">
            <div class="paper-image"><img src="images/pixel-seg.png" alt="PixISegNet"></div>
            <div class="paper-content">
              <div class="paper-title"><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-bmt.2019.0025">PixISegNet: Pixel-level Iris Segmentation Network</a></div>
              <div class="paper-authors">RR Jha, G Jaswal, <strong>S Saini</strong>, D Gupta, A Nigam</div>
              <div class="paper-venue">IET Biometrics 2019</div>
              <div class="paper-abstract">Convolutional encoder-decoder with stacked hourglass bottleneck for pixel-level iris segmentation.</div>
              <div class="paper-links"><a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-bmt.2019.0025" class="paper-link">PDF</a></div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="projects">
      <div class="section-header">
        <h2>Previous Projects</h2>
      </div>
      <div class="project-grid">
        <div class="project-card">
          <div class="project-title"><a href="https://github.com/shreshthsaini/SR-DDPM">Efficient Super-Resolution with Diffusion Models</a></div>
          <div class="project-desc">Implementation of an efficient SR3 DM for Super Resolution, exploring pre-trained diffusion models for image enhancement.</div>
        </div>
        <div class="project-card">
          <div class="project-title"><a href="https://github.com/shreshthsaini/Zero-DA">Zero-DA: Zero-shot Diffusion for Video Animation</a></div>
          <div class="project-desc">Adapts image generation models to video production using hierarchical cross-frame constraints for temporal uniformity.</div>
        </div>
        <div class="project-card">
          <div class="project-title"><a href="https://github.com/shreshthsaini/Flow-Less-VFI">Optical Flow-less Video Frame Interpolation</a></div>
          <div class="project-desc">Uses transformers with mutual self-attention between frames as a surrogate for motion estimation in VFI.</div>
        </div>
        <div class="project-card">
          <div class="project-title"><a href="https://github.com/shreshthsaini/Inverse_Problems_CSE_393P">Problems on General Inverse and Solution</a></div>
          <div class="project-desc">Collection of implementations and analyses of various inverse problem-solving techniques (CSE 393P).</div>
        </div>
        <div class="project-card">
          <div class="project-title"><a href="https://github.com/shreshthsaini/UBR-UnBiased-and-Robust-Recidivism-Prediction">UBR: Unbiased Recidivism Prediction</a></div>
          <div class="project-desc">ML techniques to mitigate bias in recidivism predictions towards gender and racial/ethnic groups.</div>
        </div>
        <div class="project-card">
          <div class="project-title"><a href="https://www.taylorfrancis.com/chapters/edit/10.1201/9781003003489-12/">Iris Segmentation in the Wild</a></div>
          <div class="project-desc">Book chapter on encoder-decoder based deep learning techniques for iris segmentation in unconstrained environments.</div>
        </div>
      </div>
    </section>

    <section class="section" id="experience">
      <div class="section-header">
        <h2>Experience</h2>
      </div>
      <ul class="experience-list">
        <li><span class="exp-date">Jun - Oct 2025</span> - <strong>Student Researcher</strong>, <a href="https://research.google/">Google Research - LUMA Team</a>, Mountain View</li>
        <li><span class="exp-date">Aug 2022 - Present</span> - <strong>Graduate Research Assistant</strong>, <a href="https://live.ece.utexas.edu/">LIVE, UT Austin</a></li>
        <li><span class="exp-date">Jun - Aug 2024</span> - <strong>Applied Scientist Intern</strong>, <a href="https://www.amazon.science/">Amazon - Perception Team</a>, Seattle</li>
        <li><span class="exp-date">Jan - May 2024</span> - <strong>Research Intern</strong>, <a href="https://www.alibabagroup.com/en/global/home">Alibaba Group</a>, Sunnyvale</li>
        <li><span class="exp-date">Jan 2023 - Jan 2024</span> - <strong>Co-Founder</strong>, Short-X, Austin</li>
        <li><span class="exp-date">Feb - Jun 2022</span> - <strong>Machine Learning Engineer</strong>, <a href="https://biomind.ai/">BioMind</a>, Singapore</li>
        <li><span class="exp-date">Aug 2020 - Dec 2021</span> - <strong>Research Engineer - AI</strong>, <a href="https://arkrayusa.com/">Arkray Inc.</a></li>
      </ul>
    </section>

    <section class="section" id="services">
      <div class="section-header">
        <h2>Academic Service</h2>
      </div>
      <div class="services-text">
        <p><strong>Reviewer:</strong> ICLR (2025, 2026), ICML (2025, 2026), ECCV (2026), CVPR (2025, 2026), IEEE Trans. on Multimedia (2024, 2025), WACV (2025, 2026), TIP (2025)</p>
        <p style="margin-top: 12px;"><strong>Leadership:</strong> Assistant Director, LIVE at UT Austin (2025-Present)</p>
      </div>
    </section>

    <footer>
      <div class="footer-map">
        <a href="https://clustrmaps.com/site/1c5ak" title="ClustrMaps">
          <img src="//www.clustrmaps.com/map_v2.png?d=jgy4unsMrAZelCO909ghCVFsfJl5bSYJhOWARYTbZ-o&amp;cl=ffffff" alt="Visitor Map" />
        </a>
      </div>
      <p class="footer-text">Â© 2026 Shreshth Saini - Built with care</p>
    </footer>
  </div>

  <script>
    window.addEventListener('scroll', function() {
      var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
      var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
      var scrolled = (winScroll / height) * 100;
      document.getElementById('scrollIndicator').style.width = scrolled + '%';
    });
    
    var sections = document.querySelectorAll('section[id]');
    var navLinks = document.querySelectorAll('.nav-links a');
    
    window.addEventListener('scroll', function() {
      var current = '';
      sections.forEach(function(section) {
        var sectionTop = section.offsetTop;
        if (scrollY >= sectionTop - 100) {
          current = section.getAttribute('id');
        }
      });
      
      navLinks.forEach(function(link) {
        link.classList.remove('active');
        if (link.getAttribute('href').includes(current)) {
          link.classList.add('active');
        }
      });
    });
    
    var showAllUpdates = false;
    function toggleUpdates() {
      var moreUpdates = document.getElementById('moreUpdates');
      var btn = document.getElementById('updatesBtn');
      showAllUpdates = !showAllUpdates;
      
      if (showAllUpdates) {
        moreUpdates.style.display = 'flex';
        btn.textContent = 'Show Less';
      } else {
        moreUpdates.style.display = 'none';
        btn.textContent = 'Show More';
      }
    }
    
    function createParticles() {
      var container = document.getElementById('particles');
      var particleCount = 15;
      
      for (var i = 0; i < particleCount; i++) {
        var particle = document.createElement('div');
        particle.className = 'particle';
        particle.style.left = Math.random() * 100 + '%';
        particle.style.top = Math.random() * 100 + '%';
        particle.style.animationDelay = Math.random() * 20 + 's';
        particle.style.animationDuration = (15 + Math.random() * 15) + 's';
        container.appendChild(particle);
      }
    }
    
    createParticles();
  </script>
</body>
</html>
