<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Shreshth Saini | AI Research Scientist</title>
  <meta name="author" content="Shreshth Saini">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Shreshth Saini - PhD Candidate at UT Austin, specializing in Generative AI, Diffusion Models, and Video Quality Assessment">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/UT_profile.png">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
  
  <style>
    * {
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    }
    
    body {
      background: #fafafa;
      color: #333;
      line-height: 1.7;
      cursor: none;
    }
    
    name {
      font-size: 32px;
      font-weight: 600;
      color: #1a1a1a;
      letter-spacing: -0.5px;
    }
    
    heading {
      font-size: 20px;
      font-weight: 500;
      color: #2c3e50;
      letter-spacing: -0.3px;
    }
    
    papertitle {
      font-size: 15px;
      font-weight: 500;
      color: #1a1a1a;
    }
    
    a {
      color: #2563eb;
      text-decoration: none;
      transition: color 0.2s;
    }
    
    a:hover {
      color: #1d4ed8;
    }
    
    .nav-links {
      position: sticky;
      top: 0;
      background: rgba(250, 250, 250, 0.95);
      backdrop-filter: blur(10px);
      padding: 12px 0;
      z-index: 100;
      border-bottom: 1px solid #eee;
      text-align: center;
      margin-bottom: 20px;
    }
    
    .nav-links a {
      margin: 0 15px;
      font-size: 13px;
      font-weight: 500;
      color: #555;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }
    
    .nav-links a:hover {
      color: #2563eb;
    }
    
    .opportunity-banner {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      padding: 16px 20px;
      border-radius: 12px;
      margin-bottom: 20px;
      box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    }
    
    .opportunity-banner strong {
      color: white;
    }
    
    .opportunity-banner a {
      color: #ffd700;
      font-weight: 500;
    }
    
    .section-title {
      font-size: 18px;
      font-weight: 500;
      color: #2c3e50;
      border-bottom: 2px solid #e5e7eb;
      padding-bottom: 8px;
      margin-bottom: 15px;
    }
    
    .category-title {
      font-size: 14px;
      font-weight: 500;
      color: #6b7280;
      text-transform: uppercase;
      letter-spacing: 1px;
      margin: 25px 0 15px 0;
      padding-left: 10px;
      border-left: 3px solid #667eea;
    }
    
    .paper-img {
      width: 140px;
      height: 100px;
      object-fit: cover;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    
    .paper-venue {
      font-size: 13px;
      color: #059669;
      font-weight: 500;
    }
    
    .paper-venue-review {
      font-size: 13px;
      color: #d97706;
      font-style: italic;
    }
    
    .paper-links a {
      font-size: 12px;
      background: #f3f4f6;
      padding: 4px 10px;
      border-radius: 4px;
      margin-right: 6px;
      color: #4b5563;
      transition: all 0.2s;
    }
    
    .paper-links a:hover {
      background: #2563eb;
      color: white;
    }
    
    .update-item {
      padding: 6px 0;
      font-size: 14px;
      border-bottom: 1px solid #f3f4f6;
    }
    
    .update-date {
      font-weight: 500;
      color: #6b7280;
      min-width: 90px;
      display: inline-block;
    }
    
    .blog-section {
      background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
      color: white;
      padding: 25px;
      border-radius: 12px;
      margin: 25px 0;
      text-align: center;
      box-shadow: 0 4px 15px rgba(240, 147, 251, 0.3);
    }
    
    .blog-section h3 {
      margin: 0 0 10px 0;
      font-size: 20px;
      font-weight: 500;
    }
    
    .blog-section p {
      margin: 0;
      opacity: 0.9;
      font-size: 14px;
    }
    
    .blog-section a {
      color: #ffd700;
      font-weight: 500;
    }
    
    button {
      background: #f3f4f6;
      border: none;
      padding: 8px 16px;
      border-radius: 6px;
      font-size: 13px;
      cursor: pointer;
      color: #4b5563;
      transition: all 0.2s;
    }
    
    button:hover {
      background: #e5e7eb;
    }
    
    #more, #moreExp, #moreServices { display: none; }
    
    /* Floating particles */
    .particle {
      position: fixed;
      width: 8px;
      height: 8px;
      background: radial-gradient(circle, rgba(102, 126, 234, 0.6) 0%, transparent 70%);
      border-radius: 50%;
      pointer-events: none;
      z-index: 1;
      transition: transform 0.3s ease-out;
    }
    
    /* Custom cursor */
    #cursor {
      position: fixed;
      width: 20px;
      height: 20px;
      border: 2px solid #667eea;
      border-radius: 50%;
      pointer-events: none;
      transform: translate(-50%, -50%);
      z-index: 9999;
      transition: all 0.15s ease-out;
      mix-blend-mode: difference;
    }
    
    #cursor-dot {
      position: fixed;
      width: 5px;
      height: 5px;
      background: #667eea;
      border-radius: 50%;
      pointer-events: none;
      transform: translate(-50%, -50%);
      z-index: 9999;
    }
    
    body:hover #cursor {
      transform: translate(-50%, -50%) scale(1);
    }
    
    a:hover ~ #cursor, button:hover ~ #cursor {
      transform: translate(-50%, -50%) scale(1.5);
      background: rgba(102, 126, 234, 0.1);
    }
  </style>
</head>

<body>
  <!-- Floating Particles Container -->
  <div id="particles"></div>
  
  <!-- Navigation -->
  <div class="nav-links">
    <a href="#about">About</a>
    <a href="#updates">Updates</a>
    <a href="#publications">Research</a>
    <a href="#experience">Experience</a>
    <a href="#projects">Projects</a>
  </div>

  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          
          <!-- About Section -->
          <table id="about" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Shreshth Saini</name>
                  </p>
                  
                  <div class="opportunity-banner">
                    <strong>ðŸ“¢ Open to Opportunities:</strong> I am actively seeking <strong>full-time Research Scientist</strong> positions in industry research labs, starting <strong>2026</strong>. My expertise spans generative AI, multimodal learning, and perceptual quality assessment. <a href="mailto:saini.2@utexas.edu">Let's connect â†’</a>
                  </div>
                  
                  <p style="font-size: 15px;">
                    I am a final-year PhD candidate at the <a href="https://live.ece.utexas.edu/">Laboratory of Image and Video Engineering (LIVE)</a>, <a href="https://www.utexas.edu/">UT Austin</a>, advised by <a href="https://www.ece.utexas.edu/people/faculty/alan-bovik">Prof. Alan C. Bovik</a> (IEEE/NAI Fellow, Primetime Emmy Award). My research focuses on <strong>Theoretical Foundations of Generative Models</strong>â€”Rectified Flows, Diffusion, and MLLMsâ€”with applications in efficient sampling, perceptual quality assessment, and inverse problems.
                  </p>
                  
                  <p style="font-size: 15px;">
                    Previously, I was a <strong>Student Researcher</strong> at <span style="color:#4285f4;">Google Research - LUMA Team</span> (Junâ€“Oct 2025). I currently serve as <strong>Assistant Director</strong> at LIVE, UT Austin. My PhD research is conducted in collaboration with <span style="color:#FF0000;">YouTube</span>/<span style="color:#4285f4;">Google</span> Media Algorithms.
                  </p>
                  
                  <p style="font-size: 15px;">
                    Prior to my PhD, I worked as Research Engineer-AI at Arkray, Inc. (Kyoto) and ML Engineer at BioMind AI (Singapore), developing AI solutions for medical imaging.
                  </p>
                  
                  <p style="font-size: 15px;">
                    I received my B.Tech in Electrical Engineering from <a href="https://www.iitj.ac.in/">IIT Jodhpur</a>, with research mentorship from <a href="https://www.mornin-feng.com/">Prof. Mengling Feng</a> (NUS), <a href="https://faculty.iitmandi.ac.in/~aditya/">Prof. Aditya Nigam</a> (IIT Mandi), and <a href="https://research.iitj.ac.in/researcher/anil-kumar-tiwari">Prof. Anil K. Tiwari</a> (IIT Jodhpur).
                  </p>
                  
                  <p style="text-align:center; font-size: 14px;">
                    <a href="mailto:saini.2@utexas.edu">Email</a> Â· 
                    <a href="https://scholar.google.co.in/citations?user=OpZ-5K4AAAAJ&hl=en">Scholar</a> Â· 
                    <a href="https://www.linkedin.com/in/shreshthfeel/">LinkedIn</a> Â· 
                    <a href="https://x.com/shreshthsaini">X</a> Â· 
                    <a href="https://github.com/shreshthsaini">GitHub</a> Â· 
                    <a href="data/CV_Shreshth_Saini_2026.pdf">CV</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:35%;max-width:35%">
                  <img style="width:100%;max-width:100%;border-radius:12px;box-shadow:0 4px 20px rgba(0,0,0,0.1)" alt="profile photo" src="images/shreshth-portrait.jpeg">
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Technical Blog Section -->
          <div class="blog-section">
            <h3>ðŸ§ª The Diffusion Lab</h3>
            <p>Technical deep-dives on generative models, diffusion theory, and cutting-edge AI research. <a href="blogs.html">Explore â†’</a></p>
          </div>

          <!-- Updates Section -->
          <table id="updates" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p class="section-title">Latest Updates</p>
                  
                  <div class="update-item"><span class="update-date">Jan 2026</span> ðŸ“¢ Open to full-time Research Scientist roles starting 2026!</div>
                  <div class="update-item"><span class="update-date">Jan 2026</span> Serving as reviewer for <strong>ICML 2026</strong> and <strong>ECCV 2026</strong></div>
                  <div class="update-item"><span class="update-date">Oct 2025</span> Completed Student Researcher position at Google Research - LUMA Team</div>
                  <div class="update-item"><span class="update-date">Sept 2025</span> Rectified CFG++ accepted to <strong>NeurIPS 2025</strong>!</div>
                  <div class="update-item"><span class="update-date">Jun 2025</span> Joined Google Research as Student Researcher in LUMA team</div>
                  <div class="update-item"><span class="update-date">May 2025</span> CHUG paper accepted at <strong>IEEE ICIP 2025</strong></div>
                  <div class="update-item"><span class="update-date">May 2025</span> LGDM paper accepted at <strong>ICML 2025</strong>!</div>
                  <div class="update-item"><span class="update-date">Mar 2025</span> Appointed <strong>Assistant Director</strong> of LIVE at UT Austin</div>
                  <span id="dots"></span>
                  <span id="more">
                    <div class="update-item"><span class="update-date">Jun 2024</span> Joined Amazon as Applied Scientist-II Intern</div>
                    <div class="update-item"><span class="update-date">Jan 2024</span> Joined Alibaba US as Research Intern</div>
                    <div class="update-item"><span class="update-date">Nov 2023</span> Paper accepted at WACV 2024 Workshop</div>
                    <div class="update-item"><span class="update-date">Aug 2022</span> Started PhD at LIVE, UT Austin</div>
                  </span>
                  <p></p>
                  <button onclick="myFunction()" id="myBtn">Show More</button>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Research Publications -->
          <table id="publications" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p class="section-title">Research Publications</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Discrete Diffusion -->
          <p class="category-title">Discrete Diffusion</p>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/tabes-mdm.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <papertitle>TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models</papertitle>
                  <br>
                  <strong>S Saini</strong>, A Saha, B Adsumilli, N Birkbeck, Y Wang, AC Bovik
                  <br>
                  <span class="paper-venue-review">Under Review â€” ICML 2026</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="#">PDF</a>
                    <a href="#">ArXiv</a>
                    <a href="#">Code</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">We introduce TABES, a novel trajectory-aware entropy steering mechanism for masked diffusion models that improves token prediction through adaptive backward sampling guided by information-theoretic principles.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Continuous Diffusion -->
          <p class="category-title">Continuous Diffusion</p>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/ITM-DM.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <papertitle>LumaFlux: Lifting 8-Bit Worlds to HDR Reality with Physically-Guided Diffusion Transformers</papertitle>
                  <br>
                  <strong>S Saini</strong>, H Gedik, N Birkbeck, Y Wang, B Adsumilli, AC Bovik
                  <br>
                  <span class="paper-venue-review">Under Review â€” CVPR 2026</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="#">PDF</a>
                    <a href="#">ArXiv</a>
                    <a href="#">Project</a>
                    <a href="#">Code</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">LumaFlux introduces physically-guided diffusion transformers for inverse tone mapping, lifting standard 8-bit content to HDR with physically accurate luminance expansion and color reproduction.</p>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/rect-cfg.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://shreshthsaini.github.io/Rectified-CFGpp/"><papertitle>Rectified-CFG++ for Flow-based Models</papertitle></a>
                  <br>
                  <strong>S Saini</strong>, S Gupta, AC Bovik
                  <br>
                  <span class="paper-venue">NeurIPS 2025</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://shreshthsaini.github.io/Rectified-CFGpp/data/pdfs/Rect_CFGpp_Neurips-compressed-names.pdf">PDF</a>
                    <a href="https://neurips.cc/virtual/2025/poster/118333">NeurIPS</a>
                    <a href="https://shreshthsaini.github.io/Rectified-CFGpp/">Project</a>
                    <a href="https://github.com/shreshthsaini/Rectified-CFGpp">Code</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">Rectified CFG++ enhances conditional generation with Rectified Flow models through adaptive latent trajectory correction, improving visual coherence and prompt alignment.</p>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/pcdm.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="data/camera_ready_LGDM.pdf"><papertitle>LGDM: Latent Guidance in Diffusion Models for Perceptual Evaluations</papertitle></a>
                  <br>
                  <strong>S Saini</strong>, R Liao, Y Ye, AC Bovik
                  <br>
                  <span class="paper-venue">ICML 2025</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://openreview.net/pdf?id=lIdcR7vU76">PDF</a>
                    <a href="https://arxiv.org/abs/2506.00327">ArXiv</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">Exploiting diffusion model priors for perceptually-consistent image quality assessment, achieving better alignment with human perception.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- MLLMs -->
          <p class="category-title">Multimodal Large Language Models</p>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/ugc-hdr-mllm.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <papertitle>Seeing Beyond 8-bits: Subjective and Objective Quality Assessment of HDR-UGC Videos</papertitle>
                  <br>
                  <strong>S Saini</strong>, B Chen, N Birkbeck, B Adsumilli, AC Bovik
                  <br>
                  <span class="paper-venue-review">Under Review â€” CVPR 2026</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="#">PDF</a>
                    <a href="#">ArXiv</a>
                    <a href="#">Project</a>
                    <a href="#">Code</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">Comprehensive framework for HDR-UGC video quality assessment combining large-scale subjective study with HDR-native multimodal reasoning.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- VQA -->
          <p class="category-title">Video Quality Assessment</p>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/brightrate.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://brightvqa.github.io/BrightVQ/"><papertitle>BrightRate: Quality Assessment for User-Generated HDR Videos</papertitle></a>
                  <br>
                  <strong>S Saini</strong>, B Chen, N Birkbeck, Y Wang, B Adsumilli, AC Bovik
                  <br>
                  <span class="paper-venue">WACV 2026</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://brightvqa.github.io/BrightVQ/static/pdfs/BrightRate.pdf">PDF</a>
                    <a href="https://brightvqa.github.io/BrightVQ/">Project</a>
                    <a href="https://github.com/brightvqa/BrightVQ/">Code</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">No-reference quality assessment framework for UGC HDR videos addressing luminance adaptation and dynamic range artifacts.</p>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/hidrovqa.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/WACV2024W/VAQ/papers/Saini_HIDRO-VQA_High_Dynamic_Range_Oracle_for_Video_Quality_Assessment_WACVW_2024_paper.pdf"><papertitle>Contrastive HDR-VQA: Deep Contrastive Learning for HDR Video Quality</papertitle></a>
                  <br>
                  <strong>S Saini</strong>, A Saha, AC Bovik
                  <br>
                  <span class="paper-venue">WACV 2024</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://arxiv.org/abs/2311.11059">ArXiv</a>
                    <a href="https://github.com/shreshthsaini/HIDRO-VQA-DATA">Data</a>
                    <a href="https://github.com/shreshthsaini/HIDRO-VQA">Code</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">Deep contrastive representation learning for HDR video quality assessment with state-of-the-art prediction performance.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Datasets -->
          <p class="category-title">Datasets & Benchmarks</p>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/chug.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://shreshthsaini.github.io/CHUG/"><papertitle>CHUG: Crowdsourced User-Generated HDR Video Quality Dataset</papertitle></a>
                  <br>
                  <strong>S Saini</strong>, N Birkbeck, Y Wang, B Adsumilli, AC Bovik
                  <br>
                  <span class="paper-venue">IEEE ICIP 2025</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://shreshthsaini.github.io/CHUG/static/pdfs/chug.pdf">PDF</a>
                    <a href="https://ieeexplore.ieee.org/abstract/document/11084488">IEEE</a>
                    <a href="https://shreshthsaini.github.io/CHUG/">Project</a>
                    <a href="https://github.com/shreshthsaini/CHUG">Code</a>
                  </div>
                  <p style="font-size:13px;color:#666;margin-top:10px">Large-scale crowdsourced dataset for HDR video quality with diverse real-world content.</p>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/prime-air.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <papertitle>Prime-EditBench: Real World Benchmark for Image/Video Editing with Diffusion</papertitle>
                  <br>
                  <strong>S Saini</strong>, P Korus, S Jin
                  <br>
                  <span style="font-size:13px;color:#6b7280;">Amazon Internal Preprint</span>
                  <p style="font-size:13px;color:#666;margin-top:10px">Standardized benchmark for evaluating diffusion-based image and video editing in real-world scenarios.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Experience Section -->
          <table id="experience" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p class="section-title">Experience</p>
                  <ul style="font-size:14px;line-height:2">
                    <li><strong>Junâ€“Oct 2025:</strong> Student Researcher, <a href="https://research.google/">Google Research - LUMA Team</a></li>
                    <li><strong>Mar 2025â€“Present:</strong> Assistant Director, <a href="https://live.ece.utexas.edu/">LIVE, UT Austin</a></li>
                    <li><strong>Aug 2022â€“Present:</strong> Graduate Research Assistant, LIVE (w/ YouTube/Google)</li>
                    <li><strong>Junâ€“Aug 2024:</strong> Applied Scientist-II Intern, <a href="https://www.amazon.science/">Amazon</a></li>
                    <li><strong>Janâ€“May 2024:</strong> Research Intern, <a href="https://www.alibabagroup.com/">Alibaba</a></li>
                    <li><strong>Febâ€“Jun 2022:</strong> ML Engineer, BioMind AI</li>
                    <li><strong>Aug 2020â€“Dec 2021:</strong> Research Engineer-AI, Arkray Inc.</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Services -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p class="section-title">Professional Service</p>
                  <p style="font-size:14px;">
                    <strong>Conference Reviewer:</strong> NeurIPS, ICLR, ICML, CVPR, ECCV, WACV (2025â€“2026)<br>
                    <strong>Journal Reviewer:</strong> IEEE TIP, IEEE TMM<br>
                    <strong>Leadership:</strong> Assistant Director, LIVE at UT Austin
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Other Projects -->
          <table id="projects" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p class="section-title">Other Projects</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/srdm.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://github.com/shreshthsaini/SR-DDPM"><papertitle>Efficient Super-Resolution with Fine-Tuning Diffusion Models</papertitle></a>
                  <br>
                  <span style="font-size:13px;color:#666;">S Saini, Y Chen, KS Durbha</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://github.com/shreshthsaini/SR-DDPM">GitHub</a>
                    <a href="https://github.com/shreshthsaini/SR-DDPM/blob/main/An%20Efficient%20Approach%20to%20Super-Resolution%20with%20Fine-Tuning%20Diffusion%20Models.pdf">PDF</a>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/zero-da.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://github.com/shreshthsaini/Zero-DA"><papertitle>Zero-DA: Zero-shot Diffusion for Video Animation</papertitle></a>
                  <br>
                  <span style="font-size:13px;color:#666;">S Saini, KS Durbha</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://github.com/shreshthsaini/Zero-DA">GitHub</a>
                    <a href="https://github.com/shreshthsaini/Zero-DA/blob/master/paper/Zero-DA.pdf">PDF</a>
                  </div>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/flowless.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://github.com/shreshthsaini/Flow-Less-VFI"><papertitle>Optical Flow-less Video Frame Interpolation</papertitle></a>
                  <br>
                  <span style="font-size:13px;color:#666;">S Saini, KS Durbha</span>
                  <br>
                  <div class="paper-links" style="margin-top:8px">
                    <a href="https://github.com/shreshthsaini/Flow-Less-VFI">GitHub</a>
                    <a href="https://github.com/shreshthsaini/Flow-Less-VFI/blob/main/Digital_Video___Report.pdf">PDF</a>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Earlier Publications -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p class="section-title">Earlier Publications (Medical AI)</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/mslae.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/9565760"><papertitle>(M)SLAe-Net: Multi-Scale Multi-Level Attention for Retinal Vessel Segmentation</papertitle></a>
                  <br>
                  <strong>S Saini</strong>, G Agrawal
                  <br>
                  <span class="paper-venue">IEEE ISBI 2021, IEEE ICHI 2021</span>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/b-segnet.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/3450439.3451873"><papertitle>B-SegNet: Branched-SegMentor Network for Skin Lesion Segmentation</papertitle></a>
                  <br>
                  <strong>S Saini</strong>, YS Jeon, M Feng
                  <br>
                  <span class="paper-venue">ACM CHIL 2021</span>
                </td>
              </tr>
              <tr>
                <td style="padding:15px;width:25%;vertical-align:middle">
                  <img src='images/pixel-seg.png' class="paper-img">
                </td>
                <td style="padding:15px;width:75%;vertical-align:middle">
                  <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-bmt.2019.0025"><papertitle>PixISegNet: Pixel-level Iris Segmentation Network</papertitle></a>
                  <br>
                  RR Jha, G Jaswal, <strong>S Saini</strong>, D Gupta, A Nigam
                  <br>
                  <span class="paper-venue">IET Biometrics 2019</span>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Footer -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;text-align:center">
                  <a href="https://clustrmaps.com/site/1c5ak" title="ClustrMaps">
                    <img src="//www.clustrmaps.com/map_v2.png?d=jgy4unsMrAZelCO909ghCVFsfJl5bSYJhOWARYTbZ-o&cl=ffffff" style="max-width:150px;opacity:0.7"/>
                  </a>
                  <p style="font-size:12px;color:#999;margin-top:15px">Â© 2026 Shreshth Saini</p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>

  <!-- Custom Cursor -->
  <div id="cursor"></div>
  <div id="cursor-dot"></div>

  <script>
    // Smooth scroll for navigation
    document.querySelectorAll('.nav-links a').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        target.scrollIntoView({ behavior: 'smooth', block: 'start' });
      });
    });

    // Updates toggle
    function myFunction() {
      var dots = document.getElementById("dots");
      var moreText = document.getElementById("more");
      var btnText = document.getElementById("myBtn");
      if (dots.style.display === "none") {
        dots.style.display = "inline";
        btnText.innerHTML = "Show More";
        moreText.style.display = "none";
      } else {
        dots.style.display = "none";
        btnText.innerHTML = "Show Less";
        moreText.style.display = "inline";
      }
    }

    // Custom cursor
    const cursor = document.getElementById('cursor');
    const cursorDot = document.getElementById('cursor-dot');
    
    document.addEventListener('mousemove', (e) => {
      cursor.style.left = e.clientX + 'px';
      cursor.style.top = e.clientY + 'px';
      cursorDot.style.left = e.clientX + 'px';
      cursorDot.style.top = e.clientY + 'px';
    });

    document.addEventListener('mousedown', () => {
      cursor.style.transform = 'translate(-50%, -50%) scale(0.8)';
    });

    document.addEventListener('mouseup', () => {
      cursor.style.transform = 'translate(-50%, -50%) scale(1)';
    });

    // Cursor hover effects
    document.querySelectorAll('a, button').forEach(el => {
      el.addEventListener('mouseenter', () => {
        cursor.style.transform = 'translate(-50%, -50%) scale(1.5)';
        cursor.style.background = 'rgba(102, 126, 234, 0.1)';
      });
      el.addEventListener('mouseleave', () => {
        cursor.style.transform = 'translate(-50%, -50%) scale(1)';
        cursor.style.background = 'transparent';
      });
    });

    // Floating particles that avoid cursor
    const particlesContainer = document.getElementById('particles');
    const particles = [];
    const numParticles = 15;
    let mouseX = 0, mouseY = 0;

    document.addEventListener('mousemove', (e) => {
      mouseX = e.clientX;
      mouseY = e.clientY;
    });

    for (let i = 0; i < numParticles; i++) {
      const particle = document.createElement('div');
      particle.className = 'particle';
      particle.style.left = Math.random() * window.innerWidth + 'px';
      particle.style.top = Math.random() * window.innerHeight + 'px';
      particlesContainer.appendChild(particle);
      particles.push({
        el: particle,
        x: parseFloat(particle.style.left),
        y: parseFloat(particle.style.top),
        vx: (Math.random() - 0.5) * 0.5,
        vy: (Math.random() - 0.5) * 0.5
      });
    }

    function animateParticles() {
      particles.forEach(p => {
        // Move particle
        p.x += p.vx;
        p.y += p.vy;

        // Bounce off walls
        if (p.x < 0 || p.x > window.innerWidth) p.vx *= -1;
        if (p.y < 0 || p.y > window.innerHeight) p.vy *= -1;

        // Avoid cursor
        const dx = p.x - mouseX;
        const dy = p.y - mouseY;
        const dist = Math.sqrt(dx * dx + dy * dy);
        if (dist < 100) {
          const angle = Math.atan2(dy, dx);
          const force = (100 - dist) / 100;
          p.x += Math.cos(angle) * force * 3;
          p.y += Math.sin(angle) * force * 3;
        }

        p.el.style.left = p.x + 'px';
        p.el.style.top = p.y + 'px';
      });
      requestAnimationFrame(animateParticles);
    }
    animateParticles();
  </script>

</body>
</html>
