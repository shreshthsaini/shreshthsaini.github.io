<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Shreshth Saini</title>

  <meta name="author" content="Shreshth Saini">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/UT_profile.png">

  <style>
  #more {display: none;}
  </style>
  
</head>

<body>
  <nav>
    <div class="nav-container">
      <a href="index.html" class="nav-logo">Shreshth Saini</a>
      <div class="nav-links">
        <a href="#about" class="active">About</a>
        <a href="#experience">Experience</a>
        <a href="#research">Research</a>
        <a href="blogs.html">Blogs</a>
        <a href="data/CV_Shreshth_Saini_2026.pdf">CV</a>
      </div>
    </div>
  </nav>

  <div id="about"></div>
  <table class="page-content" style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shreshth Saini</name>
              </p>
              <!-- <div class="announcement announcement-hero">
                <div class="announcement-text">
                  <span class="announcement-icon">&#128227;</span>
                  <strong>Open to Opportunities:</strong> I am actively seeking full-time Research Scientist positions, starting 2026. My expertise spans generative AI, multimodal learning, post-training, and perceptual quality assessment. <span class="announcement-link">Let's connect &rarr;</span>
                </div>
              </div> -->
              <p> I am a final year PhD student at <a href="https://live.ece.utexas.edu/">Laboratory of Image and Video Engineering</a> at <a href="https://www.utexas.edu/">The University of Texas Austin</a>, advised by <a href="https://www.ece.utexas.edu/people/faculty/alan-bovik"> Prof. Alan C. Bovik </a>. My research focuses on the <strong>Theoretical Foundations of Generative Models</strong> (e.g. Flows, Diffusion, and MLLMs) and their applications in efficient sampling, image/video-quality assessment (QA), editing, and inverse problems (eg: ITM).
              I collaborate with <a style="color:#FF0000;"> YouTube</a>/<a style="color:#0091ff;"> Google</a> Media Algorithms team in my PhD. I was a Student Researcher in the LUMA team at <a style="color:#0091ff;"> Google Research</a> (Jun–Oct 2025).
              </p>              
              <p>Before starting my PhD at UT Austin, I worked as a Research Engineer-AI at <!--a href="https://www.arkray.co.jp/english/" </a>--> Arkray, inc and as Machine Learning Engineer at <!-- a href="https://biomind.ai/" </a> -->  BioMind AI. At both places, I was working on developing novel and scalable AI solutions for medical image analysis. 
              </p>
              <p>
              I received my Bachelor's degree in Electrical Engineering from the <a href="https://www.iitj.ac.in/">IIT Jodhpur</a>. I was fortunate to be advised by  <a href="https://www.mornin-feng.com/"> Prof. Menglign Feng </a>(NUS),  <a href="https://faculty.iitmandi.ac.in/~aditya/"> Prof. Aditya Nigam </a>(IIT Mandi), and <a href="https://research.iitj.ac.in/researcher/anil-kumar-tiwari"> Prof. Anil K. Tiwari</a>(IIT Jodhpur) throughout my undergraduate research. 
              </p>
              <p style="text-align:center">
                <a href="mailto:saini.2@utexas.edu">Contact</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=OpZ-5K4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shreshthfeel/">LinkedIn</a> &nbsp/&nbsp           
                <a href="https://x.com/shreshthsaini">X</a> &nbsp/&nbsp 
                <a href="https://github.com/shreshthsaini">GitHub</a> &nbsp/&nbsp <!-- &nbsp/&nbsp -->
                <a href="data/CV_Shreshth_Saini_2026.pdf">CV (2026)</a>
              </p>
              
            </td>
            <td class="profile-cell" style="padding:2.5%;width:40%;max-width:40%;">
              <a href="images/snow.jpg"><img alt="profile photo" src="images/snow.jpg" class="hoverZoomLink profile-photo"></a>
            </td>
          </tr>
        </tbody></table>

  
<!-- Announcements========================================================================================== -->   
  <div class="announcement announcement-hero">
                <div class="announcement-text">
                  <span class="announcement-icon">&#128227;</span>
                  <strong>Open to Opportunities.</strong><br>
                  
                  I am actively seeking full-time Research Scientist positions, starting 2026. My expertise spans generative AI, multimodal learning, post-training, and perceptual quality assessment. <a href="mailto:saini.2@utexas.edu" class="announcement-link">Let's connect &rarr;</a>
                </div>
              </div>
  <div class="announcement announcement-blog">
                <div class="announcement-text">
                  <strong>Scratch Pad:</strong> Essays/Posts and Technical Notes & Insights live on the blog. <a href="blogs.html" class="announcement-link">Explore &rarr;</a>
                </div>
              </div>



<!-- Updates========================================================================================== -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading style="color:#8B0000;">Updates</heading>
          
            <ul>
              <li><span class="date-col"><b>Jan 2026:</b></span> <b>Open to full-time Research Scientist opportunities.</b></li>
              <li><span class="date-col"><b>Jan 2026:</b></span> Serving as a reviewer for <b>ICML 2026</b> and <b>ECCV 2026</b>.</li>
              <li><span class="date-col"><b>Dec 2025:</b></span> BrightRate accepted in <b>WACV 2026</b> for oral presentation.</li>
              <li><span class="date-col"><b>Dec 2025:</b></span> Completed Progress review.</li>
              <li><span class="date-col"><b>Sep 2025:</b></span> Rectified CFG++ accepted to <b>NeurIPS 2025</b>!</li>
              <li><span class="date-col"><b>Jun 2025:</b></span> Excited to join <b>Google Research</b> as Student Researcher in the LUMA team!</li>
              <!--<li><span class="date-col"><b>May 2025:</b></span> Formed PhD committee - officially advancing to candidacy!</li>  -->             
	      <li><span class="date-col"><b>May 2025:</b></span> Paper accepted in IEEE International Conference on Image Processing, <b>IEEE ICIP 2025!</b></li>
	      <li><span class="date-col"><b>May 2025:</b></span> Paper accepted in 42nd International Conference on Machine Learning <b>(ICML) 2025!</b></li>
	      <li><span class="date-col"><b>May 2025:</b></span> Paper accepted in 3rd Workshop on Generative Models for Computer Vision, <b>CVPR 2025!</b></li>
              <li><span class="date-col"><b>Mar 2025:</b></span> Honored to be appointed as <b>Assistant Director</b> of <a href="https://live.ece.utexas.edu/">LIVE at UT Austin</a>!</li>
              <li><span class="date-col"><b>Jun 2024:</b></span> Joined <b>Amazon</b> as Applied Scientist-II Intern in the Perception team!</li>
              <li><span class="date-col"><b>Jan 2024:</b></span> Joined <b>Alibaba US</b> as Research Intern to work on diffusion models!</li>
              <li><span class="date-col"><b>Nov 2023:</b></span> Paper accepted in 3rd Workshop on Image/Video/Audio Quality in CV and Gen AI, WACV 2024!</li>
              <span id="dots"></span><span id="more">
              <li><span class="date-col"><b>Jun 2023:</b></span> Completed first-ever large-scale HDR subjective perceptual quality study on Amazon Mechanical Turk!</li>
              <li><span class="date-col"><b>Aug 2022:</b></span> Joined <a href="https://live.ece.utexas.edu/">LIVE at UT Austin</a> as PhD student under Prof. Alan C. Bovik!</li>
              <li><span class="date-col"><b>Aug 2022:</b></span> Awarded prestigious Engineering Graduate Fellowship until 2027!</li>
              <li><span class="date-col"><b>Feb 2022:</b></span> Joined BioMind, Singapore as Research Engineer/Machine Learning Engineer.</li>
              <li><span class="date-col"><b>Aug 2020:</b></span> Started as Research Engineer (AI) at Arkray, Inc.</li>
              <li><span class="date-col"><b>May 2019:</b></span> Research Assistant at National University of Singapore (NUS) under Prof. Mengling "Mornin" Feng.</li>
              <li><span class="date-col"><b>Aug 2018:</b></span> Undergraduate Researcher at Image Processing and Computer Vision Lab, IIT Jodhpur under Prof. Anil Kumar Tiwari.</li>
              <li><span class="date-col"><b>May 2018:</b></span> Research Intern at The Multimedia Analytics, Networks and Systems Lab, IIT Mandi under Prof. Aditya Nigam.</li>
              </span>
            </ul>
              <p></p>
              <button onclick="myFunction()" id="myBtn">More</button>
            
            <p></p>
          </td>
        </tr>
        </tbody>
      </table>
      
      <script>
        function myFunction() {
          var dots = document.getElementById("dots");
          var moreText = document.getElementById("more");
          var btnText = document.getElementById("myBtn");

          if (dots.style.display === "none") {
            dots.style.display = "inline";
            btnText.innerHTML = "More"; 
            moreText.style.display = "none";
          } else {
            dots.style.display = "none";
            btnText.innerHTML = "Less"; 
            moreText.style.display = "inline";
          }
        }
        </script>


<!-- Research========================================================================================== -->
<table id="experience" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading style="color:#8B0000;">Research & Development Experience</heading>

    <ul>
      <li><span class="date-col-wide"><b>Jun 2025 - Oct 2025:</b></span> Student Researcher, <a href="https://research.google/">Google Research - LUMA Team</a>, Mountain View </li>
      <li><span class="date-col-wide"><b>Aug 2022 - Present:</b></span> Graduate Research Assistant, <a href="https://live.ece.utexas.edu/research/LIVE_UGC_HDR/index.html">Laboratory for Image and Video Engineering, UT Austin</a></li>  
      <li><span class="date-col-wide"><b>Jun 2024 - Aug 2024:</b></span> Applied Scientist Intern, <a href="https://www.amazon.science/">Amazon - Perception Team</a>, Seattle</li>
      <li><span class="date-col-wide"><b>Jan 2024 - May 2024:</b></span> Research Intern, <a href="https://www.alibabagroup.com/en/global/home">Alibaba Group</a>, Sunnyvale</li>
      <li><span class="date-col-wide"><b>Jan 2023 - Jan 2024:</b></span> Co-Founder, Short-X, Austin</li>
      <li><span class="date-col-wide"><b>Feb 2022 - Jun 2022:</b></span> Machine Learning Engineer, <a href="https://biomind.ai/">BioMind</a>, Singapore</li>
      <li><span class="date-col-wide"><b>Aug 2020 - Dec 2021:</b></span> Research Engineer - AI, <a href="https://arkrayusa.com/">Arkray Inc.</a> (Remote)</li>
      <span id="dotsExp"></span><span id="moreExp">
      <li><span class="date-col-wide"><b>May 2019 - Aug 2019:</b></span> Research Assistant, <a href="https://www.nus.edu.sg/">National University of Singapore</a></li>
      <li><span class="date-col-wide"><b>Aug 2018 - Aug 2020:</b></span> Undergraduate Researcher, Image Processing and Computer Vision Lab, IIT Jodhpur</li>
      <li><span class="date-col-wide"><b>May 2018 - Aug 2018:</b></span> Research Intern, The Multimedia Analytics, Networks and Systems Lab, IIT Mandi</li>
      </span>
    </ul>
    <button onclick="toggleExperience()" id="expBtn" style="margin-top: 15px; padding: 5px 10px; background-color: #f5f5f5; border: 1px solid #ddd; cursor: pointer;">Extended Version</button>
    <div id="extendedExp" style="display: none; margin-top: 20px;">
      <p>
        <strong>Applied Scientist Intern</strong> | Amazon - Perception Team<br>
        <span style="color: #666;">Seattle, Washington | Jun 2024 – Aug 2024</span>
      </p>
      <ul>
        <li>Worked with the Perception team on large-scale synthetic data generation</li>
        <li>Developed novel edit-bench and T2I-based diffusion model for consistent image/video editing and generation</li>
        <li>Aiming to conduct Image+Video editing challenge and workshop</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Research Intern</strong> | Alibaba Group<br>
        <span style="color: #666;">Sunnyvale, California | Jan 2024 – May 2024</span>
      </p>
      <ul>
        <li>Developed generalizable and robust Vision Model-based Video Quality Assessment (VQA) methods</li>
        <li>Using Diffusion Model priors as perceptual consistency for IQA (Paper: under review)</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Co-Founder</strong> | Short-X<br>
        <span style="color: #666;">Austin, Texas | Jan 2023 – Jan 2024</span>
      </p>
      <ul>
        <li>Short-X aims to automate the arduous task of making short-form contents from traditional long-form content</li>
        <li>Built core AI models and pipelines for Short-X, working on transcription, extracting semantically meaningful and unique highlights, removing pauses, identifying speaker and smart vertical cropping</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Graduate Research Assistant</strong> | Laboratory for Image and Video Engineering, UT Austin<br>
        <span style="color: #666;">Austin, Texas | Aug 2022 – Present</span>
      </p>
      <ul>
        <li>Developing scalable vision models for HDR videos for tasks like ITM/TM, gamut expansion & quality assessment</li>
        <li>Created the largest HDR-SDR dataset for short-form videos (publicly available)</li>
        <li>Developing video quality assessment methods for HDR videos, which uses Non-Linear expansion of extremes of sub-level luminance</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Machine Learning Engineer</strong> | BioMind (Products)<br>
        <span style="color: #666;">Singapore, Singapore | Feb 2022 – Jun 2022</span>
      </p>
      <ul>
        <li>Developed SOTA multimodal DL models for segmentation and classification of 25+ tumor/non-tumor classes</li>
        <li>Exploited TFRecords for memory-intense 4D datasets and proposed multi-task model for tumor predictions</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Research Engineer – AI</strong> | Arkray, Inc.<br>
        <span style="color: #666;">Kyoto, Japan (Remote) | Aug 2020 – Dec 2021</span>
      </p>
      <ul>
        <li>Proposed semi-supervised DL models to learn from a large chunk of the private unlabelled and noisy 2D datasets</li>
        <li>Deployed models for products: UrineSediment Analyzer, and automated BodyFluid Analyzer (Aution EYE)</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Research Assistant</strong> | National University of Singapore<br>
        <span style="color: #666;">Singapore | May 2019 – Jul 2019</span><br>
        <span style="color: #666;">Supervisor: Dr. Mengling 'Mornin' Feng</span>
      </p>
      <ul>
        <li>Developed novel deep learning architecture for large-scale public health datasets</li>
        <li>Published SOTA results with low cost for skin lesion analysis</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Undergraduate Researcher</strong> | Image Processing and Computer Vision Lab, IIT Jodhpur<br>
        <span style="color: #666;">Jodhpur, India | Aug 2018 – Aug 2020</span><br>
        <span style="color: #666;">Supervisor: Dr. Anil Kumar Tiwari</span>
      </p>
      <ul>
        <li>Worked on developing ML methods aimed for AI-based diagnosis and treatment support</li>
        <li>Developed DL models for retinal vessel & skin lesion segmentation, and diagnosis of left-atrium in 3D GE-MRIs</li>
      </ul>
      
      <p style="margin-top: 15px;">
        <strong>Research Intern</strong> | The Multimedia Analytics, Networks and Systems Lab, IIT Mandi<br>
        <span style="color: #666;">Mandi, India | May 2018 – Jul 2018</span><br>
        <span style="color: #666;">Supervisor: Dr. Aditya Nigam</span>
      </p>
      <ul>
        <li>Developed novel CNN model for iris segmentation which uses cascaded hourglass modules at the bottleneck of encoder-decoder design</li>
      </ul>
    </div>
  </td>

</tbody></table>

<script>
function experienceFunction() {
var dots = document.getElementById("dotsExp");
var moreText = document.getElementById("moreExp");
var btnText = document.getElementById("expBtn");

if (dots.style.display === "none") {
  dots.style.display = "inline";
  btnText.innerHTML = "More"; 
  moreText.style.display = "none";
} else {
  dots.style.display = "none";
  btnText.innerHTML = "Less"; 
  moreText.style.display = "inline";
}
}

function toggleExperience() {
var extendedContent = document.getElementById("extendedExp");
var btnText = document.getElementById("expBtn");

if (extendedContent.style.display === "none") {
  extendedContent.style.display = "block";
  btnText.innerHTML = "Collapse";
} else {
  extendedContent.style.display = "none";
  btnText.innerHTML = "Extended Version";
}
}
</script>


<!-- Publications==========================================================================================-->

    <table id="research" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading style="color:#8B0000;">Research Publications</heading><br>
      </td>
    </tr>
  </tbody></table>

  <table id="projects" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
        <heading style="font-style: italic; font-size: large">Discrete Diffusion</heading>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/mdm-sampling.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Model</papertitle></a>
        <br>
        <strong>S Saini</strong>, A Saha, B Adsumilli, N Birkbeck, Y Wang, AC Bovik.
        <br>
        Under Review - ICML 2026
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>We introduce TABES, a novel trajectory-aware entropy steering mechanism for masked diffusion models that improves token prediction through adaptive backward sampling guided by information-theoretic principles.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
        <heading style="font-style: italic; font-size: large">MLLMs</heading>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/hdr-mllm.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>Seeing Beyond 8bits: Subjective and Objective Quality Assessment of HDR-UGC Videos</papertitle></a>
        <br>
        <strong>S Saini</strong>, B Chen, N Birkbeck, B Adsumilli, AC Bovik.
        <br>
        Under Review - CVPR 2026
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>We present a large-scale subjective study and objective QA baselines for HDR-UGC videos, enabling reliable assessment of real-world HDR content.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
        <heading style="font-style: italic; font-size: large">Continuous Diffusion</heading>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/lumaflux.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>LumaFlux: Lifting 8-Bit Worlds to HDR Reality with Physically-Guided Diffusion Transformers</papertitle></a>
        <br>
        <strong>S Saini</strong>, H Gedik, N Birkbeck, Y Wang, B Adsumilli, AC Bovik.
        <br>
        Under Review - CVPR 2026
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>LumaFlux introduces physically-guided diffusion transformers for inverse tone mapping, lifting standard 8-bit content to HDR with physically accurate luminance expansion and color reproduction.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/rect-cfg.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>Rectified CFG++ for Flow Based Models</papertitle></a>
        <br>
        <strong>S Saini</strong>, S Gupta, AC Bovik.
        <br>
        Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025) (also at 3rd CVPR Workshop on Generative Models)
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>Rectified CFG++ enhances conditional image generation with Rectified Flow models by adaptively correcting the latent trajectory. This method improves visual coherence and alignment with text prompts, outperforming existing samplers in generation quality and efficiency.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/pcdm.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>LGDM: Latent Guidance in Diffusion Models for Perceptual Evaluations</papertitle></a>
        <br>
        <strong>S Saini</strong>, R Liao, Y Ye, AC Bovik.
        <br>
        International Conference on Machine Learning (ICML) 2025
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>This study investigates the exploitation of diffusion model priors to achieve perceptual consistency in image quality assessment. By leveraging the inherent priors learned by diffusion models, the assessment of image quality is made more aligned with human perception, leading to more accurate and reliable evaluations.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
        <heading style="font-style: italic; font-size: large">VQA</heading>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/brightrate.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>BrightRate: Quality Assessment for User-Generated HDR Videos (Oral)</papertitle></a>
        <br>
         <strong>S Saini</strong>, B Chen, N Birkbeck, B Adsumilli, AC Bovik
        <br>
        IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026 (Oral)
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>BrightRate is designed for quality assessment in user-generated HDR videos, focusing on capture variability and perceptual fidelity.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/hidrovqa.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>Contrastive HDR-VQA: Deep Contrastive Representation Learning for HDR Video Quality Assessment</papertitle></a>
        <br>
        <strong>S Saini</strong>, A Saha, AC Bovik.
        <br>
        IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2024
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>Contrastive HDR-VQA introduces a deep contrastive representation learning approach for high dynamic range video quality assessment. By learning robust representations through contrastive learning, the method achieves state-of-the-art performance in predicting the quality of HDR videos.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle" colspan="2">
        <heading style="font-style: italic; font-size: large">Dataset &amp; Benchmarks</heading>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/chug.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>CHUG: Crowdsourced User-Generated HDR Video Quality Dataset</papertitle></a>
        <br>
        <strong>S Saini</strong>, N Birkbeck, B Adsumilli, AC Bovik
        <br>
        IEEE International Conference on Image Processing (ICIP) 2025
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>CHUG is a crowdsourced dataset for HDR video quality, addressing the need for diverse, real-world content. It aids in developing more accurate and robust quality assessment models.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/prime-air.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>Prime-EditBench: A Real-World Benchmark for Image and Video Editing Tasks using Diffusion Models</papertitle></a>
        <br>
        <strong>S Saini</strong>, P Korus, S Jin.
        <br>
        Amazon Prime Air / Perception (Internal)
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>Prime-EditBench is a real-world benchmark designed to evaluate image and video editing with diffusion models, enabling standardized assessment of editing performance.</p>
      </td>
    </tr>
  </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading style="color:#8B0000;">Old Publications</heading><br>
    <p> </p>
    <heading style="font-style: italic; font-size: large">(Medical AI)</heading>
  </td>
</tr>
</tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/mslae.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>(M)SLAe-Net: Multi-Scale Multi-Level Attention Embedded Network for Retinal Vessel Segmentation</papertitle></a>
        <br>
        <strong>S Saini</strong>, G Agrawal.
        <br>
        IEEE ISBI 2021 &amp; IEEE ICHI 2021 (Oral)
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>M2SLAe-Net introduces a multi-scale multi-level attention embedded network for improved retinal vessel segmentation. By integrating attention mechanisms at multiple scales and levels, the network achieves enhanced accuracy and robustness in segmenting retinal vessels, aiding in the diagnosis of various eye diseases.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/b-segnet.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>B-SegNet: Branched SegMentor Network for Skin Lesion Segmentation</papertitle></a>
        <br>
        <strong>S Saini</strong>, YS Jeon, M Feng.
        <br>
        ACM CHIL 2021 (Oral)
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>B-SegNet introduces a branched SegMentor network for accurate skin lesion segmentation. By employing a branched architecture, the network effectively captures both local and global features of skin lesions, leading to improved segmentation performance and aiding in the diagnosis of skin cancer.</p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/detector-segnet.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>Detector-SegMentor Network for Skin Lesion Localization and Segmentation</papertitle></a>
        <br>
        <strong>S Saini</strong>, D Gupta, AK Tiwari.
        <br>
        NCVPRIPG 2019 (Oral)
        <br>
        <a href="#">PDF</a> / <a href="#">ArXiv</a> / <a href="#">Page</a> / <a href="#">Code</a>
        <br>
        <p>This paper presents a detector and SegMentor network for simultaneous skin lesion localization and segmentation. The network combines detection and segmentation tasks to provide a comprehensive solution for skin lesion analysis, enabling accurate localization and precise segmentation of lesions for improved diagnostic accuracy.</p>
      </td>
    </tr>
  </tbody></table>

  <!-- Journals ==========================================================================================-->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Journals</heading>
      </td>
    </tr>
  </tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/pixel-seg.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>PixISegNet: pixel-level iris segmentation network using convolutional encoder–decoder with stacked hourglass bottleneck</papertitle></a>
        <br>
        RR Jha, G Jaswal, <strong>S Saini</strong>, et al.
        <br>
        IET Biometrics, 2019
        <br>
        <a href="#">PDF</a>
        <br>
        <p>PixISegNet introduces a pixel-level iris segmentation network that utilizes a convolutional encoder-decoder architecture with a stacked hourglass bottleneck. This network achieves precise iris segmentation by effectively capturing both local and global features, making it suitable for various biometric applications.</p>
      </td>
    </tr>
  </tbody></table>

  <!-- Book Chapters ==========================================================================================-->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Book / Book Chapters</heading>
      </td>
    </tr>
  </tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/chapter.png' class="thumb-square">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="#"><papertitle>Iris Segmentation in the Wild using Encoder-Decoder based Deep Learning Techniques</papertitle></a>
        <br>
        <strong>S Saini</strong>, et al.
        <br>
        AI and Deep Learning in Biometric Security, CRC Press, 2020
        <br>
        <a href="#">PDF</a>
        <br>
        <p>This book chapter explores the use of encoder-decoder based deep learning techniques for iris segmentation in unconstrained environments. The proposed methods effectively handle challenges such as variations in lighting, occlusion, and off-angle images, making them suitable for real-world biometric applications.</p>
      </td>
    </tr>
  </tbody></table> 



  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading style="color:#8B0000;">Projects</heading><br>
    </td>
  </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/inverse-cse.png' class="thumb-square">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/shreshthsaini/Inverse_Problems_CSE_393P"><papertitle>Problems on General Inverse and Solution (CSE 393P)</papertitle></a>
      <br>
      GitHub Repository
      <br>
      <p>This repository contains problems and solutions related to general inverse problems, as part of the CSE 393P course. It includes implementations and analyses of various inverse problem-solving techniques.</p>
    </td>
  </tr>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/srdm.png' class="thumb-square">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/shreshthsaini/SR-DDPM"><papertitle>An Efficient Approach to Super-Resolution with Fine-Tuning Diffusion Models</papertitle></a>
      <br>
      Shreshth Saini, Yu-Chih Chen, Krishna Srikar Durbha
      <br>
      <a href="https://github.com/shreshthsaini/SR-DDPM">GitHub</a> / <a href="https://github.com/shreshthsaini/SR-DDPM/blob/main/An%20Efficient%20Approach%20to%20Super-Resolution%20with%20Fine-Tuning%20Diffusion%20Models.pdf">PDF</a>
      <br>
      <p>Implementation of an efficient SR3 DM for Super Resolution. This project explores the potential of pre-trained diffusion models to enhance the generalization ability and reduce computation costs in image super-resolution tasks.</p>
    </td>
  </tr>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/zero-da.png' class="thumb-square">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/shreshthsaini/Zero-DA"><papertitle>Zero-DA: Zero-shot Diffusion Model for Video Animation</papertitle></a>
      <br>
      Shreshth Saini, Krishna Srikar Durbha
      <br>
      <a href="https://github.com/shreshthsaini/Zero-DA">GitHub</a> / <a href="https://github.com/shreshthsaini/Zero-DA/blob/master/paper/Zero-DA.pdf">PDF</a>
      <br>
      <p>Zero-shot Diffusion Model for Video Animation (Zero-DA) adapts image generation models to video production. This framework tackles the challenge of maintaining temporal uniformity across video frames using hierarchical cross-frame constraints.</p>
    </td>
  </tr>
    <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/recividism.png' class="thumb-square">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/shreshthsaini/UBR-UnBiased-and-Robust-Recidivism-Prediction?tab=readme-ov-file#ubr-unbiased-and-robust-recidivism-prediction"><papertitle>UBR-Unbiased-and-Robust-Recidivism-Prediction</papertitle></a>
      <br>
      Shreshth Saini, Albert Joe, Jiachen Wang, SayedMorteza Malaekeh
            <br>
      <a href="https://github.com/shreshthsaini/UBR-UnBiased-and-Robust-Recidivism-Prediction?tab=readme-ov-file#ubr-unbiased-and-robust-recidivism-prediction">GitHub</a> / <a href="https://github.com/shreshthsaini/UBR-UnBiased-and-Robust-Recidivism-Prediction/blob/master/Recidivism.pdf">PDF</a>
      <br>
      <p>This project aims to mitigate the inherent bias in recidivism score predictions by leveraging machine learning techniques to rectify and minimize biases towards gender and racial/ethnic groups.</p>
    </td>
  </tr>
    <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src='images/flowless.png' class="thumb-square">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/shreshthsaini/Flow-Less-VFI"><papertitle>Optical flow less video frame interpolation</papertitle></a>
      <br>
      Shreshth Saini, Krishna Srikar Durbha
      <br>
      <a href="https://github.com/shreshthsaini/Flow-Less-VFI">GitHub</a> / <a href="https://github.com/shreshthsaini/Flow-Less-VFI/blob/main/Digital_Video___Report.pdf">PDF</a>
      <br>
      <p>This project proposes the use of transformers to learn long-range interactions with mutual self-attention between frames as a surrogate for motion estimation in video frame interpolation.</p>
    </td>
  </tr>
</tbody></table>

<!-- Preprints==========================================================================================-->

       <!--  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/otm.png'>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=""><papertitle>Generative Modeling with Optimal Transport Maps</papertitle></a>
              <br>
              <strong>Litu Rout</strong>, Alexander Korotin, and Evgeny Burnaev
              <br>
              <a href="data/preprint1_2021.pdf">PDF</a> &nbsp <a href="https://arxiv.org/abs/2010.00522">ArXiv</a>
              <p></p>
              <p>While Optimal Transport (OT) cost serves as the loss for popular generative models, we demonstrate that the OT map can be used as the generative model itself. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. </p>
            </td>
         </tr>
        </tbody></table> -->



<!-- Invited Talk========================================================================================== -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Tutorials</heading>
              </td>
              </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
      <!--========================================================================================== -->
       <!--    <td>
            <ul>
              <li> <b>Jun 2021:</b> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17137">Why Adversarial Interaction Creates Non-Homogeneous Patterns: A Pseudo-Reaction-Diffusion Model for Turing Instability</a>, Knowledge Sharing Series on Artificial Intelligence: Theory and Practice, SAC, ISRO, India.
                <br>
                  <a href="data/KSS_Lecture1_02062021.pdf">[Lecture 1]</a> &nbsp <a href="data/KSS_Lecture2_09062021.pdf">[Lecture 2]</a> &nbsp <a href="data/KSS_Lecture3_16062021.pdf">[Lecture 3]</a> &nbsp <a href="data/Learning_a_Linear_Function_Approximator.html">[Demo Code 1]</a> &nbsp <a href="data/Learning_One_Layer_Neural_Network_Function_Approximator.html">[Demo Code 2]</a> &nbsp <a href="data/Learning_Two_Layer_Neural_Network_Function_Approximator.html">[Demo Code 3]</a> &nbsp <a href="data/Learning_Three_Layer_Convolutional_Neural_Network_Function_Approximator.html">[Demo Code 4]</a></li>
              <p></p>
              <li> <b>Aug 2020:</b> <b>Deep Learning: Real World Applications and Implementation Details</b>, Human Resource Development Division (HRDD), VSSC, ISRO, India.
                <br>
                  <a href="data/vssc_talk_aug_2020_lr.pdf">[Slides]</a> &nbsp <a href="data/Learning_a_Linear_Function_Approximator.html">[Demo Code 1]</a> &nbsp <a href="data/Learning_One_Layer_Neural_Network_Function_Approximator.html">[Demo Code 2]</a> &nbsp <a href="data/Learning_Two_Layer_Neural_Network_Function_Approximator.html">[Demo Code 3]</a> &nbsp <a href="data/Learning_Three_Layer_Convolutional_Neural_Network_Function_Approximator.html">[Demo Code 4]</a></li>
              <p></p>
              <li> <b>Apr 2020:</b> <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w11/Rout_S2A_Wasserstein_GAN_With_Spatio-Spectral_Laplacian_Attention_for_Multi-Spectral_Band_CVPRW_2020_paper.html">S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for Multi-Spectral Band Synthesis</a>, Earth, Ocean, Atmosphere, Planetary Sciences and Applications Area (EPSA), SAC, ISRO, India.
                <br>
                 <a href="data/309-talk.pdf">[Slides]</a></li>
              <p></p>
              <li> <b>Apr 2020:</b> <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w11/Rout_Monte-Carlo_Siamese_Policy_on_Actor_for_Satellite_Image_Super_Resolution_CVPRW_2020_paper.html"> Monte-Carlo Siamese Policy on Actor for Satellite Image Super Resolution</a>, Earth, Ocean, Atmosphere, Planetary Sciences and Applications Area (EPSA), SAC, ISRO, India.
                <br>
                 <a href="data/324-talk.pdf">[Slides]</a></li>
              <p></p>
              <li> <b>Mar 2020:</b> <b>Global and Local Residual Learning for Spatio-Spectral Synthesis of SWIR Band using Multi-Sensor Concurrent Datasets</b>, National Remote Sensing Agencies, SAC, ISRO, India. </li>
              <p></p>
              <li> <b>Jul 2018:</b> <b>Understanding Artificial Neural Networks to Deep Learning</b>, Mohandas College of Engineering and Technology <a href="https://mcetonline.com/">(MCET)</a>, Thiruvananthapuram, Kerala, India.</li>
            </ul>
            <p></p> -->
            <!--==========================================================================================-->
			<!-- Page visitors -->
			<!--==========================================================================================-->
			<!-- map widget -->
			       <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=12kD2ArDBbodu3fRjVPsyL6j_66fWZ0yr4SIFZMXDgA'></script> -->
			<!-- flag widget -->
				<!-- <a href="https://info.flagcounter.com/Kaa5"><img src="https://s04.flagcounter.com/count2/Kaa5/bg_FFFFFF/txt_000000/border_CCCCCC/columns_4/maxflags_20/viewers_3/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a> -->
			        
        <!--   </td>
        </tbody></table> -->



<!-- Service========================================================================================== -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading style="color:#8B0000;">Professional Service</heading>
    <div class="service-block">
      <div class="service-title">Reviewer and Program Committee Member</div>
      <div class="service-list">
        <div class="service-row"><span class="service-item">ICLR</span><span class="service-years">2025, 2026</span></div>
        <div class="service-row"><span class="service-item">ICML</span><span class="service-years">2025, 2026</span></div>
        <div class="service-row"><span class="service-item">CVPR</span><span class="service-years">2025, 2026</span></div>
        <div class="service-row"><span class="service-item">ICCV/ECCV</span><span class="service-years">2025, 2026</span></div>
        <div class="service-row"><span class="service-item">WACV</span><span class="service-years">2024, 2025, 2026</span></div>
        <div class="service-row"><span class="service-item">IEEE TIP</span><span class="service-years">2025, 2026</span></div>
        <div class="service-row"><span class="service-item">IEEE Trans. on Multimedia</span><span class="service-years">2024, 2025, 2026</span></div>
        <div class="service-row"><span class="service-item">TMLR</span><span class="service-years">2025, 2026</span></div>
      </div>
    </div>
    <div class="service-block">
      <div class="service-title">Other Service</div>
      <div class="service-list">
        <div class="service-row"><span class="service-item">Assistant Director, LIVE at UT Austin</span><span class="service-years">2025-Present</span></div>
        <div class="service-row"><span class="service-item">Volunteer, Internal Workshop on Deep Learning (IWDL), India</span><span class="service-years">2018</span></div>
        <div class="service-row"><span class="service-item">Established and ran LAMBDA Lab at IITJ</span><span class="service-years">2018-2020</span></div>
        <div class="service-row"><span class="service-item">Overall Head, Entrepreneurship and Innovation Cell at IITJ</span><span class="service-years">2018-2019</span></div>
        <div class="service-row"><span class="service-item">Assistant Head, Counselling Services at IITJ</span><span class="service-years">2018-2019</span></div>
      </div>
    </div>
  </td>
</tr>
</tbody>
</table>


<!--==========================================================================================-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <!-- <div> Thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the template.</div> -->
              <a href="https://clustrmaps.com/site/1c5ak"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=jgy4unsMrAZelCO909ghCVFsfJl5bSYJhOWARYTbZ-o&cl=ffffff" /></a>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  </body>



</html>
