<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>variational | meliorate</title>
    <link>https://shreshthsaini.github.io/tags/variational/</link>
      <atom:link href="https://shreshthsaini.github.io/tags/variational/index.xml" rel="self" type="application/rss+xml" />
    <description>variational</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020</copyright><lastBuildDate>Mon, 31 Aug 2020 20:06:52 +0530</lastBuildDate>
    <image>
      <url>https://shreshthsaini.github.io/img/icon-192.png</url>
      <title>variational</title>
      <link>https://shreshthsaini.github.io/tags/variational/</link>
    </image>
    
    <item>
      <title>Variational AutoEncoder</title>
      <link>https://shreshthsaini.github.io/post/variational-autoencoder/</link>
      <pubDate>Mon, 31 Aug 2020 20:06:52 +0530</pubDate>
      <guid>https://shreshthsaini.github.io/post/variational-autoencoder/</guid>
      <description>

&lt;p&gt;In the last few years deep learning practictioners have developed a huge interest in deep generative models. Credit for this can be given to availability of huge datasets, computational capacity, efficient optimization methods and well-designed networks. With all these, generative model are now able to produce realistic contents be it images, texts or voice/music. In this post I have tried to present my understandig of one such generative approach : Variational Auto-Encoder.&lt;/p&gt;

&lt;h3 id=&#34;auto-encoders&#34;&gt;Auto-Encoders&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Before we actually dive into the Variational autoencoders, lets have a clarity about what are autoencoders.&lt;/em&gt;&lt;/strong&gt;
Autoencoder are used for data compression and other derivative tasks such as segmentation, noise reduction, feature manipulation in image, etc.. Autoencoder consists of two part namely &lt;strong&gt;&lt;em&gt;Encoder&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Decoder&lt;/em&gt;&lt;/strong&gt;. Encoder(E) takes the input(X) and produces corresponding latent space representation(E(X)), output of encoder is then fed to decoder(D) which gives us our final output(D(E(X))). Autoencoder are trained supervisingly (X,Y). With convolutional autoencoder we can extract high dimensional representation of our data from latent space. No doubt autoencoder works better for limited applications. Encoders tend to map the data to a discontinuous latent space (i.e segregation of clusters at bottleneck). This specifically creates issue when we deploy these models for generative tasks which might require sampling and/or interpolation from latent space and which might create generate unrealistic outputs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;autoencoder.jpg&#34; alt=&#34;autoencoder-sample.jpg&#34; /&gt;
&lt;em&gt;Fig. 1: Convolutional Autoencoder.&lt;a href=&#34;#1&#34;&gt;[1]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;variational-auto-encoders-vaes&#34;&gt;Variational Auto-Encoders(VAEs)&lt;/h3&gt;

&lt;p&gt;Having understood the basics of autoencoder and its structure now we can move towards the real thing. It it to be notes that latent space representations from simple autoencoder can not be used in generation task as it gives the a rather unrealistic output. Reason being distribution of latent space is often not continuous. Variational autonencoders introduce additional layers at bottleneck of the network to extract the probablistic distribution of the latent space (mean and variance), in additional a KL-Divergence loss &lt;a href=&#34;#2&#34;&gt;[2]&lt;/a&gt; is also used to bring the data distribution close up for interpolation inbetween the classes (generation). In general results of VAE are blurry, novel loss functions such as generative loss &lt;a href=&#34;#3&#34;&gt;[3]&lt;/a&gt; or perceptual loss &lt;a href=&#34;#4&#34;&gt;[4]&lt;/a&gt; can be used to remove the blurriness.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vae.jpg&#34; alt=&#34;vae-sample.jpg&#34; /&gt;
&lt;em&gt;Fig. 2: Convolutional Variational Autoencoder. Low dimensional representation feature is sampled from learned distribution at bottlecneck. Unit gaussian distribution depicts convergence of learned distribution towards normal distribution.&lt;a href=&#34;#5&#34;&gt;[5]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;about-mean-and-standard-variation&#34;&gt;About Mean and Standard Variation&lt;/h3&gt;

&lt;p&gt;&amp;rdquo;&amp;ldquo;&amp;rdquo;
They form the parameters of a vector of random variables of length n, with the i th element of μ and σ being the mean and standard deviation of the i th random variable, X i, from which we sample, to obtain the sampled encoding which we pass onward to the decoder.&lt;/p&gt;

&lt;p&gt;This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling.&lt;/p&gt;

&lt;p&gt;Intuitively, the mean vector controls where the encoding of an input should be centered around, while the standard deviation controls the “area”, how much from the mean the encoding can vary. As encodings are generated at random from anywhere inside the “circle” (the distribution), the decoder learns that not only is a single point in latent space referring to a sample of that class, but all nearby points refer to the same as well. This allows the decoder to not just decode single, specific encodings in the latent space (leaving the decodable latent space discontinuous), but ones that slightly vary too, as the decoder is exposed to a range of variations of the encoding of the same input during training.&lt;/p&gt;

&lt;p&gt;The model is now exposed to a certain degree of local variation by varying the encoding of one sample, resulting in smooth latent spaces on a local scale, that is, for similar samples. Ideally, we want overlap between samples that are not very similar too, in order to interpolate between classes. However, since there are no limits on what values vectors μ and σ can take on, the encoder can learn to generate very different μ for different classes, clustering them apart, and minimize σ, making sure the encodings themselves don’t vary much for the same sample (that is, less uncertainty for the decoder). This allows the decoder to efficiently reconstruct the training data.&lt;/p&gt;

&lt;p&gt;What we ideally want are encodings, all of which are as close as possible to each other while still being distinct, allowing smooth interpolation, and enabling the construction of new samples.&lt;/p&gt;

&lt;p&gt;In order to force this, we introduce the Kullback–Leibler divergence (KL divergence[2]) into the loss function. The KL divergence between two probability distributions simply measures how much they diverge from each other. Minimizing the KL divergence here means optimizing the probability distribution parameters (μ and σ) to closely resemble that of the target distribution.&lt;/p&gt;

&lt;p&gt;For VAEs, the KL loss is equivalent to the sum of all the KL divergences between the component Xi~N(μi, σi²) in X, and the standard normal[3]. It’s minimized when μi = 0, σi = 1.&lt;/p&gt;

&lt;p&gt;Intuitively, this loss encourages the encoder to distribute all encodings (for all types of inputs, eg. all MNIST numbers), evenly around the center of the latent space. If it tries to “cheat” by clustering them apart into specific regions, away from the origin, it will be penalized.&lt;/p&gt;

&lt;p&gt;Now, using purely KL loss results in a latent space results in encodings densely placed randomly, near the center of the latent space, with little regard for similarity among nearby encodings. The decoder finds it impossible to decode anything meaningful from this space, simply because there really isn’t any meaning.&lt;/p&gt;

&lt;p&gt;Optimizing using pure KL divergence loss&lt;/p&gt;

&lt;p&gt;Optimizing the two together, however, results in the generation of a latent space which maintains the similarity of nearby encodings on the local scale via clustering, yet globally, is very densely packed near the latent space origin (compare the axes with the original).&lt;/p&gt;

&lt;p&gt;Optimizing using both reconstruction loss and KL divergence loss&lt;/p&gt;

&lt;p&gt;Intuitively, this is the equilibrium reached by the cluster-forming nature of the reconstruction loss, and the dense packing nature of the KL loss, forming distinct clusters the decoder can decode. This is great, as it means when randomly generating, if you sample a vector from the same prior distribution of the encoded vectors, N(0, I), the decoder will successfully decode it. And if you’re interpolating, there are no sudden gaps between clusters, but a smooth mix of features a decoder can understand.
&amp;ldquo;&amp;rdquo;&amp;rdquo;&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;REFERENCES:&lt;/h3&gt;

&lt;p&gt;&lt;a id=&#34;1&#34;&gt;[1]&lt;/a&gt;
&lt;a href=&#34;https://www.cs.umd.edu/sites/default/files/scholarly_papers/Larrue,%20Tara_1801.pdf&#34; target=&#34;_blank&#34;&gt;Tara Larrue and Xiaoxu Meng and Chang-Mu Han.
Denoising Videos with Convolutional Autoencoders A Comparison of Autoencoder Architectures, 2018.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;2&#34;&gt;[2]&lt;/a&gt;
&lt;a href=&#34;https://doi.org/10.1007/978-3-642-04898-2_327&#34; target=&#34;_blank&#34;&gt;Joyce, James M., Lovric, Miodrag.
Kullback-Leibler Divergence. International Encyclopedia of Statistical Science
Springer Berlin Heidelberg. 720&amp;ndash;722, 978-3-642-04898-2. 2011
DOI:10.&lt;sup&gt;1007&lt;/sup&gt;&amp;frasl;&lt;sub&gt;978&lt;/sub&gt;-3-642-04898-2_327&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;3&#34;&gt;[3]&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/1512.09300&#34; target=&#34;_blank&#34;&gt;Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther.
Autoencoding beyond pixels using a learned similarity metric. 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;4&#34;&gt;[4]&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/1610.00291&#34; target=&#34;_blank&#34;&gt;Xianxu Hou, Linlin Shen, Ke Sun, Guoping Qiu.
Deep Feature Consistent Variational Autoencoder. 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;5&#34;&gt;[5]&lt;/a&gt;
&lt;a href=&#34;https://iq.opengenus.org/types-of-autoencoder/&#34; target=&#34;_blank&#34;&gt;Abhinav Prakash.
Different types of Autoencoders. opengenus.org.
University of Massachusetts, Amherst.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
